{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Text-to-Image Diffusion Demo\n",
        "This notebook shows a tiny diffusion pipeline for text-to-image using the provided shape sketches. I keep the comments short and simple.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Imports\n",
        "I install the basic libraries and bring the imports I need for tensors, plotting, and loading the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If your runtime misses these libraries, run this cell once.\n",
        "# In many university labs this is enough to get numpy, torch, and plotting tools.\n",
        "!pip install -q numpy torch torchvision matplotlib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading and prep\n",
        "I load the tiny shape dataset (circle, square, triangle) and pair each image with a simple text prompt. I resize/normalize to a small square so training stays fast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ShapeTextDataset(Dataset):\n",
        "    def __init__(self, data_dir=\"data\", img_size=32):\n",
        "        self.samples = []\n",
        "        prompt_map = {\n",
        "            \"circle\": \"a simple black circle on white\",\n",
        "            \"square\": \"a simple black square on white\",\n",
        "            \"triangle\": \"a simple black triangle on white\",\n",
        "        }\n",
        "        for name in [\"circle\", \"square\", \"triangle\"]:\n",
        "            imgs = np.load(f\"{data_dir}/{name}.npy\")  # expected shape (N,H,W) or (N,H,W,1)\n",
        "            if imgs.ndim == 4:\n",
        "                imgs = imgs[..., 0]\n",
        "            # ensure float32 and simple resize via numpy slicing if bigger than target\n",
        "            imgs = imgs.astype(np.float32)\n",
        "            # basic center crop/resize: pick middle square then resize with torch later\n",
        "            self.samples.extend([(img, prompt_map[name]) for img in imgs])\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, prompt = self.samples[idx]\n",
        "        img = torch.tensor(img)\n",
        "        # add channel dim and simple resize via interpolation\n",
        "        if img.ndim == 2:\n",
        "            img = img.unsqueeze(0)\n",
        "        img = img.unsqueeze(0) if img.ndim == 2 else img\n",
        "        img = torch.nn.functional.interpolate(img.unsqueeze(0), size=(self.img_size, self.img_size), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "        img = (img / 255.0) * 2 - 1  # scale to [-1,1]\n",
        "        return img, prompt\n",
        "\n",
        "def get_dataloader(batch_size=32, img_size=32):\n",
        "    dataset = ShapeTextDataset(img_size=img_size)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataloader = get_dataloader(batch_size=16, img_size=32)\n",
        "print(\"Total samples:\", len(dataloader.dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick look at data\n",
        "I plot a few samples to make sure the loader works and the prompts line up with the sketches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs, prompts = next(iter(dataloader))\n",
        "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
        "for i in range(6):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(((imgs[i].squeeze() + 1) / 2).clamp(0,1), cmap=\"gray\")\n",
        "    ax.set_title(prompts[i][:10] + \"...\")\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diffusion helpers\n",
        "I set up the beta schedule, sinusoidal time embeddings, and helper functions for the forward diffusion process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "T = 200\n",
        "betas = cosine_beta_schedule(T)\n",
        "alphas = 1.0 - betas\n",
        "alpha_cum = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t.cpu()).float().to(t.device)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    sqrt_ac = torch.sqrt(extract(alpha_cum, t, x_start.shape))\n",
        "    sqrt_om = torch.sqrt(1 - extract(alpha_cum, t, x_start.shape))\n",
        "    return sqrt_ac * x_start + sqrt_om * noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text encoder and tiny U-Net\n",
        "I encode the text with a GRU into a single vector and feed that into a light U-Net. The text and time embeddings modulate the conv blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=128, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(1000, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        emb = self.embedding(tokens)\n",
        "        _, h = self.rnn(emb)\n",
        "        return self.proj(h.squeeze(0))\n",
        "\n",
        "\n",
        "def tokenize(prompts: List[str], vocab=None, max_len=16):\n",
        "    if vocab is None:\n",
        "        vocab = {}\n",
        "    token_lists = []\n",
        "    for text in prompts:\n",
        "        tokens = []\n",
        "        for word in text.lower().split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab) + 1\n",
        "            tokens.append(vocab[word])\n",
        "        token_lists.append(tokens[:max_len])\n",
        "    max_len = max(len(t) for t in token_lists)\n",
        "    padded = []\n",
        "    for t in token_lists:\n",
        "        padded.append(t + [0] * (max_len - len(t)))\n",
        "    return torch.tensor(padded, dtype=torch.long), vocab\n",
        "\n",
        "\n",
        "def sinusoidal_embedding(n, d):\n",
        "    pos = torch.arange(n)[:, None]\n",
        "    dim = torch.arange(d)[None, :]\n",
        "    angle = pos / (10000 ** (2 * (dim // 2) / d))\n",
        "    emb = torch.zeros((n, d))\n",
        "    emb[:, 0::2] = torch.sin(angle[:, 0::2])\n",
        "    emb[:, 1::2] = torch.cos(angle[:, 1::2])\n",
        "    return emb\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class TinyUNet(nn.Module):\n",
        "    def __init__(self, base=32, time_dim=128, text_dim=128):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_dim, time_dim), nn.SiLU())\n",
        "        self.text_proj = nn.Linear(text_dim, time_dim)\n",
        "\n",
        "        self.down1 = ConvBlock(1, base)\n",
        "        self.down2 = ConvBlock(base, base * 2)\n",
        "        self.to_vec = nn.Conv2d(base * 2, base * 4, 3, padding=1)\n",
        "        self.up1 = ConvBlock(base * 4, base * 2)\n",
        "        self.up2 = ConvBlock(base * 2, base)\n",
        "        self.out = nn.Conv2d(base, 1, 1)\n",
        "\n",
        "    def forward(self, x, t_embed, text_embed):\n",
        "        t = self.time_mlp(t_embed)[:, :, None, None]\n",
        "        txt = self.text_proj(text_embed)[:, :, None, None]\n",
        "\n",
        "        d1 = self.down1(x)\n",
        "        d1 = d1 + t + txt\n",
        "        d2 = self.down2(nn.functional.avg_pool2d(d1, 2))\n",
        "        d2 = d2 + t + txt\n",
        "        mid = self.to_vec(d2)\n",
        "        mid = mid + t + txt\n",
        "        u1 = nn.functional.interpolate(self.up1(mid), scale_factor=2, mode=\"nearest\")\n",
        "        u2 = self.up2(u1 + d1)\n",
        "        return self.out(u2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "I pick a small number of steps so it runs on a laptop. The loss trains the U-Net to predict the noise added at each timestep.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_encoder = TextEncoder().to(device)\n",
        "model = TinyUNet().to(device)\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(text_encoder.parameters()), lr=1e-3)\n",
        "vocab = {}\n",
        "\n",
        "\n",
        "def get_time_embedding(timesteps, dim=128):\n",
        "    emb = sinusoidal_embedding(max(T, timesteps.max().item() + 1), dim).to(device)\n",
        "    return emb[timesteps]\n",
        "\n",
        "\n",
        "def p_losses(x0, prompts):\n",
        "    b = x0.shape[0]\n",
        "    t = torch.randint(0, T, (b,), device=device).long()\n",
        "    noise = torch.randn_like(x0)\n",
        "    x_noisy = q_sample(x0, t, noise)\n",
        "    global vocab\n",
        "    token_batch, vocab = tokenize(list(prompts), vocab)\n",
        "    token_batch = token_batch.to(device)\n",
        "    text_emb = text_encoder(token_batch)\n",
        "    t_emb = get_time_embedding(t)\n",
        "    pred = model(x_noisy, t_emb, text_emb)\n",
        "    return nn.functional.mse_loss(pred, noise)\n",
        "\n",
        "\n",
        "def train(epochs=3):\n",
        "    model.train()\n",
        "    text_encoder.train()\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(dataloader, desc=f\"epoch {epoch+1}\")\n",
        "        for imgs, prompts in loop:\n",
        "            imgs = imgs.to(device)\n",
        "            loss = p_losses(imgs, prompts)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "train(epochs=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling from the model\n",
        "I start from random noise and apply the reverse diffusion steps while conditioning on the text prompt. A few prompts show the link between words and shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, text_emb):\n",
        "    betas_t = extract(betas, t, x.shape).to(device)\n",
        "    sqrt_one_minus_ac = torch.sqrt(1 - extract(alpha_cum, t, x.shape)).to(device)\n",
        "    sqrt_recip_alpha = torch.sqrt(1.0 / extract(alphas, t, x.shape)).to(device)\n",
        "\n",
        "    model_mean = sqrt_recip_alpha * (x - betas_t * model(x, get_time_embedding(t), text_emb) / sqrt_one_minus_ac)\n",
        "    if t.item() == 0:\n",
        "        return model_mean\n",
        "    noise = torch.randn_like(x)\n",
        "    return model_mean + torch.sqrt(betas_t) * noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(prompts: List[str], img_size=32):\n",
        "    model.eval(); text_encoder.eval()\n",
        "    token_batch, _ = tokenize(prompts, vocab)\n",
        "    token_batch = token_batch.to(device)\n",
        "    text_emb = text_encoder(token_batch)\n",
        "    b = len(prompts)\n",
        "    img = torch.randn((b, 1, img_size, img_size), device=device)\n",
        "    for i in reversed(range(T)):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), text_emb)\n",
        "    img = (img.clamp(-1, 1) + 1) / 2\n",
        "    return img.cpu()\n",
        "\n",
        "example_prompts = [\n",
        "    \"a simple black circle on white\",\n",
        "    \"a simple black square on white\",\n",
        "    \"a simple black triangle on white\",\n",
        "]\n",
        "\n",
        "samples = sample(example_prompts)\n",
        "grid = make_grid(samples, nrow=len(example_prompts), normalize=False)\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.imshow(grid.squeeze(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}