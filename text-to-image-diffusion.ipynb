{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58c58364-df01-4299-983b-d7df4bb7f627",
   "metadata": {},
   "source": [
    "# Option 1: Text-to-Image Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688863d-d443-4a75-8a85-1ce0ee908271",
   "metadata": {},
   "source": [
    "## Install \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "100dbefe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:33:02.575116Z",
     "iopub.status.busy": "2025-11-28T00:33:02.574353Z",
     "iopub.status.idle": "2025-11-28T00:33:02.578631Z",
     "shell.execute_reply": "2025-11-28T00:33:02.577795Z",
     "shell.execute_reply.started": "2025-11-28T00:33:02.575092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# If you already have torch/torchvision/numpy/matplotlib installed you can skip this cell.\n",
    "# The `-q` flag keeps output compact.\n",
    "# !pip install -q torch torchvision torchaudio matplotlib tqdm numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eef19b",
   "metadata": {},
   "source": [
    "## Imports and reproducibility\n",
    "Set seeds and pick the GPU automatically when available. The same seeds are reused in sampling for repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9eabe82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:33:02.585497Z",
     "iopub.status.busy": "2025-11-28T00:33:02.584905Z",
     "iopub.status.idle": "2025-11-28T00:33:02.600268Z",
     "shell.execute_reply": "2025-11-28T00:33:02.599544Z",
     "shell.execute_reply.started": "2025-11-28T00:33:02.585472Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae105e",
   "metadata": {},
   "source": [
    "## Data loading and prep\n",
    "The dataset ships as `data/circle.npy`, `data/triangle.npy`, and `data/square.npy`. Each array contains grayscale drawings. To keep epochs bounded around ~6k samples, the loader optionally caps each class. Light geometric jittering helps the tiny model generalize and prevents collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71396315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:33:02.601706Z",
     "iopub.status.busy": "2025-11-28T00:33:02.601506Z",
     "iopub.status.idle": "2025-11-28T00:33:02.708918Z",
     "shell.execute_reply": "2025-11-28T00:33:02.708171Z",
     "shell.execute_reply.started": "2025-11-28T00:33:02.601685Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6000 total sketches (cap 2000 per class) using 2 worker(s)\n"
     ]
    }
   ],
   "source": [
    "class ShapeTextDataset(Dataset):\n",
    "    def __init__(self, data_dir: str = \"/kaggle/input/data-circle-square-triangle\", img_size: int = 32, max_items_per_class: int = 2000, augment: bool = True):\n",
    "        self.samples: List[Tuple[np.ndarray, str]] = []\n",
    "        prompt_map = {\n",
    "            \"circle\": \"a hand-drawn circle\",\n",
    "            \"square\": \"a hand-drawn square\",\n",
    "            \"triangle\": \"a hand-drawn triangle\",\n",
    "        }\n",
    "        for name in [\"circle\", \"square\", \"triangle\"]:\n",
    "            path = os.path.join(data_dir, f\"{name}.npy\")\n",
    "            # mmap keeps load time bounded even if the raw files contain far more than the capped subset\n",
    "            arr = np.load(path, mmap_mode=\"r\")\n",
    "            arr = arr[:max_items_per_class]\n",
    "            if arr.ndim == 4:  # (N,H,W,C)\n",
    "                arr = arr[..., 0]\n",
    "            self.samples.extend([(img.astype(np.float32), prompt_map[name]) for img in arr])\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _prepare_tensor(self, img: np.ndarray) -> torch.Tensor:\n",
    "        tensor = torch.as_tensor(img, dtype=torch.float32)\n",
    "        if tensor.ndim == 1:\n",
    "            side = int(math.sqrt(tensor.numel()))\n",
    "            tensor = tensor.view(side, side)\n",
    "        if tensor.ndim == 2:\n",
    "            tensor = tensor.unsqueeze(0)\n",
    "        if tensor.shape[0] > 1:\n",
    "            tensor = tensor.mean(dim=0, keepdim=True)\n",
    "        return tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, prompt = self.samples[idx]\n",
    "        img = self._prepare_tensor(img)\n",
    "        img = TF.resize(img, [self.img_size, self.img_size])\n",
    "        if self.augment:\n",
    "            angle = random.uniform(-8, 8)\n",
    "            img = TF.rotate(img, angle, fill=0.0)\n",
    "            img = TF.affine(img, angle=0.0, translate=(random.uniform(-2, 2), random.uniform(-2, 2)), scale=1.0, shear=0.0, fill=0.0)\n",
    "        img = (img / 255.0).clamp(0, 1) * 2 - 1\n",
    "        return img, prompt\n",
    "\n",
    "\n",
    "def get_dataloader(batch_size=32, img_size=32, max_items_per_class=2000, augment=True, num_workers=None):\n",
    "    dataset = ShapeTextDataset(img_size=img_size, max_items_per_class=max_items_per_class, augment=augment)\n",
    "    if num_workers is None:\n",
    "        if os.name == \"nt\":\n",
    "            # Windows prefers the main process; worker spawn can otherwise stall the first batch\n",
    "            num_workers = 0\n",
    "        else:\n",
    "            cpu_budget = max(1, (os.cpu_count() or 2) - 1)\n",
    "            num_workers = min(2, cpu_budget)\n",
    "    persistent = num_workers > 0 and os.name != \"nt\"\n",
    "    print(f\"Loaded {len(dataset)} total sketches (cap {max_items_per_class} per class) using {num_workers} worker(s)\")\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=persistent,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "\n",
    "# Default training loader now automatically disables extra workers on Windows to keep progress bars responsive\n",
    "dataloader = get_dataloader(batch_size=16, img_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b5b51",
   "metadata": {},
   "source": [
    "### Quick look at data\n",
    "Confirm that the images align with the prompts after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f4ce3",
   "metadata": {},
   "source": [
    "### Quick sanity-check batch\n",
    "If the preview below ever hangs for more than a minute, drop `num_workers` to 0. \n",
    "The cap of 64 sketches per class and `mmap` loading keeps the first batch fast (a few seconds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef85ba16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:33:02.710249Z",
     "iopub.status.busy": "2025-11-28T00:33:02.709990Z",
     "iopub.status.idle": "2025-11-28T00:33:13.264661Z",
     "shell.execute_reply": "2025-11-28T00:33:13.263996Z",
     "shell.execute_reply.started": "2025-11-28T00:33:02.710224Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 192 total sketches (cap 64 per class) using 0 worker(s)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preview_loader = get_dataloader(batch_size=6, img_size=32, max_items_per_class=64, augment=False, num_workers=0)\n",
    "imgs, prompts = next(iter(preview_loader))\n",
    "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(((imgs[i].squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
    "    ax.set_title(prompts[i][:10] + \"...\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a879a",
   "metadata": {},
   "source": [
    "## Diffusion helpers\n",
    "Cosine noise schedule with sinusoidal time embeddings. The helper functions implement the forward process and sampling utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4495394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:33:13.265714Z",
     "iopub.status.busy": "2025-11-28T00:33:13.265344Z",
     "iopub.status.idle": "2025-11-28T00:33:13.274936Z",
     "shell.execute_reply": "2025-11-28T00:33:13.274250Z",
     "shell.execute_reply.started": "2025-11-28T00:33:13.265692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "T = 300\n",
    "betas = cosine_beta_schedule(T)\n",
    "alphas = 1.0 - betas\n",
    "alpha_cum = torch.cumprod(alphas, dim=0)\n",
    "alpha_cum_prev = torch.cat([torch.ones(1), alpha_cum[:-1]])\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t.cpu()).float().to(t.device)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    return extract(alpha_cum.sqrt(), t, x_start.shape) * x_start + extract((1 - alpha_cum).sqrt(), t, x_start.shape) * noise\n",
    "\n",
    "def p_mean_variance(model, x, t, t_emb, text_emb):\n",
    "    pred_noise = model(x, t_emb, text_emb)\n",
    "    beta_t = extract(betas, t, x.shape)\n",
    "    alpha_t = extract(alphas, t, x.shape)\n",
    "    alpha_cum_t = extract(alpha_cum, t, x.shape)\n",
    "    coef1 = 1 / torch.sqrt(alpha_t)\n",
    "    coef2 = beta_t / torch.sqrt(1 - alpha_cum_t)\n",
    "    mean = coef1 * (x - coef2 * pred_noise)\n",
    "    var = beta_t\n",
    "    return mean, var, pred_noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_emb, text_emb):\n",
    "    mean, var, pred_noise = p_mean_variance(model, x, t, t_emb, text_emb)\n",
    "    if (t == 0).all():\n",
    "        return mean, pred_noise\n",
    "    noise = torch.randn_like(x)\n",
    "    return mean + torch.sqrt(var) * noise, pred_noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca70e9",
   "metadata": {},
   "source": [
    "## Text encoder and tiny U-Net\n",
    "GRU-based text encoder feeds a light U-Net. Time and text embeddings are injected into each block to keep conditioning stable. An EMA copy of the network is maintained for sharper sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a19c9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:33:13.276674Z",
     "iopub.status.busy": "2025-11-28T00:33:13.276349Z",
     "iopub.status.idle": "2025-11-28T00:33:13.296077Z",
     "shell.execute_reply": "2025-11-28T00:33:13.295583Z",
     "shell.execute_reply.started": "2025-11-28T00:33:13.276656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=128, hidden_dim=192):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(1024, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        emb = self.embedding(tokens)\n",
    "        _, h = self.rnn(emb)\n",
    "        return self.proj(h.squeeze(0))\n",
    "\n",
    "def tokenize(prompts: List[str], vocab=None, max_len=16):\n",
    "    if vocab is None:\n",
    "        vocab = {}\n",
    "    token_lists = []\n",
    "    for text in prompts:\n",
    "        tokens = []\n",
    "        for word in text.lower().split():\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab) + 1\n",
    "            tokens.append(vocab[word])\n",
    "        token_lists.append(tokens[:max_len])\n",
    "    max_len = max(len(t) for t in token_lists)\n",
    "    padded = []\n",
    "    for t in token_lists:\n",
    "        padded.append(t + [0] * (max_len - len(t)))\n",
    "    return torch.tensor(padded, dtype=torch.long), vocab\n",
    "\n",
    "def sinusoidal_embedding(n, d):\n",
    "    pos = torch.arange(n)[:, None]\n",
    "    dim = torch.arange(d)[None, :]\n",
    "    angle = pos / (10000 ** (2 * (dim // 2) / d))\n",
    "    emb = torch.zeros((n, d))\n",
    "    emb[:, 0::2] = torch.sin(angle[:, 0::2])\n",
    "    emb[:, 1::2] = torch.cos(angle[:, 1::2])\n",
    "    return emb\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(1, out_ch),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class TinyUNet(nn.Module):\n",
    "    def __init__(self, base=32, time_dim=128, text_dim=192):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(nn.Linear(time_dim, time_dim), nn.SiLU())\n",
    "        self.text_mlp = nn.Sequential(nn.Linear(text_dim, text_dim), nn.SiLU())\n",
    "        cond_channels = {\"d1\": base, \"d2\": base * 2, \"mid\": base * 4, \"u1\": base * 2, \"u2\": base}\n",
    "        self.time_proj = nn.ModuleDict({k: nn.Linear(time_dim, v) for k, v in cond_channels.items()})\n",
    "        self.text_proj = nn.ModuleDict({k: nn.Linear(text_dim, v) for k, v in cond_channels.items()})\n",
    "\n",
    "        self.down1 = ConvBlock(1, base)\n",
    "        self.down2 = ConvBlock(base, base * 2)\n",
    "        self.to_vec = ConvBlock(base * 2, base * 4)\n",
    "        self.up1 = ConvBlock(base * 4, base * 2)\n",
    "        self.up2 = ConvBlock(base * 2, base)\n",
    "        self.skip_proj = nn.Conv2d(base, base * 2, 1)\n",
    "        self.out = nn.Conv2d(base, 1, 1)\n",
    "\n",
    "    def _inject(self, feat, name, t_feat, txt_feat):\n",
    "        t = self.time_proj[name](t_feat)[:, :, None, None]\n",
    "        txt = self.text_proj[name](txt_feat)[:, :, None, None]\n",
    "        return feat + t + txt\n",
    "\n",
    "    def forward(self, x, t_embed, text_embed):\n",
    "        t_feat = self.time_mlp(t_embed)\n",
    "        txt_feat = self.text_mlp(text_embed)\n",
    "\n",
    "        d1 = self.down1(x)\n",
    "        d1 = self._inject(d1, \"d1\", t_feat, txt_feat)\n",
    "        d2 = self.down2(nn.functional.avg_pool2d(d1, 2))\n",
    "        d2 = self._inject(d2, \"d2\", t_feat, txt_feat)\n",
    "        mid = self.to_vec(d2)\n",
    "        mid = self._inject(mid, \"mid\", t_feat, txt_feat)\n",
    "        u1 = nn.functional.interpolate(self.up1(mid), scale_factor=2, mode=\"nearest\")\n",
    "        skip = self.skip_proj(d1)\n",
    "        u1 = self._inject(u1 + skip, \"u1\", t_feat, txt_feat)\n",
    "        u2 = self.up2(u1)\n",
    "        u2 = self._inject(u2, \"u2\", t_feat, txt_feat)\n",
    "        return self.out(u2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139cb76",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "The model predicts the noise added at timestep *t*. EMA smoothing, gradient clipping, and a cosine learning rate schedule stabilize training. Default settings keep each epoch to the ~6k sketches cap; increase `max_items_per_class` to use the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96394d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:34:51.642186Z",
     "iopub.status.busy": "2025-11-28T00:34:51.641485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef41b4555ba4209a15bb3696374800e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 1/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 checkpoint saved to /kaggle/working/shape_diffusion_epoch_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 2/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 checkpoint saved to /kaggle/working/shape_diffusion_epoch_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 3/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 checkpoint saved to /kaggle/working/shape_diffusion_epoch_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 4/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 checkpoint saved to /kaggle/working/shape_diffusion_epoch_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 5/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 checkpoint saved to /kaggle/working/shape_diffusion_epoch_5.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 6/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 checkpoint saved to /kaggle/working/shape_diffusion_epoch_6.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 7/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 checkpoint saved to /kaggle/working/shape_diffusion_epoch_7.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 8/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 checkpoint saved to /kaggle/working/shape_diffusion_epoch_8.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 9/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 checkpoint saved to /kaggle/working/shape_diffusion_epoch_9.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 10/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 checkpoint saved to /kaggle/working/shape_diffusion_epoch_10.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 11/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 checkpoint saved to /kaggle/working/shape_diffusion_epoch_11.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 12/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 checkpoint saved to /kaggle/working/shape_diffusion_epoch_12.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f201e63bcb4a4dc6a6b7c8238ac1211c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch 13/15:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder = TextEncoder().to(device)\n",
    "model = TinyUNet().to(device)\n",
    "ema_model = TinyUNet().to(device)\n",
    "ema_model.load_state_dict(model.state_dict())\n",
    "ema_decay = 0.995\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(text_encoder.parameters()), lr=2e-4)\n",
    "vocab: Dict[str, int] = {}\n",
    "\n",
    "def get_time_embedding(timesteps, dim=128):\n",
    "    emb = sinusoidal_embedding(max(T, timesteps.max().item() + 1), dim).to(device)\n",
    "    return emb[timesteps]\n",
    "\n",
    "def update_ema(model_src, model_tgt, decay):\n",
    "    with torch.no_grad():\n",
    "        for p_src, p_tgt in zip(model_src.parameters(), model_tgt.parameters()):\n",
    "            p_tgt.mul_(decay).add_(p_src, alpha=1 - decay)\n",
    "\n",
    "def p_losses(x0, prompts):\n",
    "    b = x0.shape[0]\n",
    "    t = torch.randint(0, T, (b,), device=device).long()\n",
    "    noise = torch.randn_like(x0)\n",
    "    x_noisy = q_sample(x0, t, noise)\n",
    "    global vocab\n",
    "    token_batch, vocab = tokenize(list(prompts), vocab)\n",
    "    token_batch = token_batch.to(device)\n",
    "    text_emb = text_encoder(token_batch)\n",
    "    t_emb = get_time_embedding(t)\n",
    "    pred = model(x_noisy, t_emb, text_emb)\n",
    "    return nn.functional.mse_loss(pred, noise)\n",
    "\n",
    "def train(epochs=5, grad_clip=1.0, checkpoint_dir=\"/kaggle/working/\"):\n",
    "    global ema_model\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(dataloader), eta_min=5e-5)\n",
    "    total_steps = epochs * len(dataloader)\n",
    "    progress = tqdm(total=total_steps, desc=\"training\")\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(dataloader, desc=f\"epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        for imgs, prompts in loop:\n",
    "            imgs = imgs.to(device)\n",
    "            loss = p_losses(imgs, prompts)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(text_encoder.parameters()), grad_clip)\n",
    "            optimizer.step()\n",
    "            update_ema(model, ema_model, ema_decay)\n",
    "            lr_scheduler.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            progress.update(1)\n",
    "            progress.set_postfix(loss=loss.item(), complete=f\"{progress.n / total_steps:.0%}\")\n",
    "        checkpoint = {\"model\": model.state_dict(), \"ema\": ema_model.state_dict(), \"text\": text_encoder.state_dict(), \"vocab\": vocab}\n",
    "        epoch_path = os.path.join(checkpoint_dir, f\"shape_diffusion_epoch_{epoch+1}.pt\")\n",
    "        torch.save(checkpoint, epoch_path)\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_dir, \"shape_diffusion_latest.pt\"))\n",
    "        print(f\"Epoch {epoch+1} checkpoint saved to {epoch_path}\")\n",
    "    progress.close()\n",
    "\n",
    "# Kick off training (adjust epochs upward for higher fidelity once you are satisfied with runtime)\n",
    "train(epochs=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ad08d",
   "metadata": {},
   "source": [
    "## Sampling from the model\n",
    "Start from random noise and apply the reverse diffusion steps conditioned on a prompt. EMA weights are preferred; if training is skipped, fall back to the current model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eab72936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T00:34:38.207212Z",
     "iopub.status.busy": "2025-11-28T00:34:38.206940Z",
     "iopub.status.idle": "2025-11-28T00:34:39.479881Z",
     "shell.execute_reply": "2025-11-28T00:34:39.479006Z",
     "shell.execute_reply.started": "2025-11-28T00:34:38.207176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8889919a8c34e98a0506cb9d163c9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def sample(prompts: List[str], use_ema: bool = True):\n",
    "    model_to_use = ema_model if use_ema else model\n",
    "    model_to_use.eval()\n",
    "    global vocab\n",
    "    token_batch, vocab = tokenize(prompts, vocab)\n",
    "    token_batch = token_batch.to(device)\n",
    "    text_emb = text_encoder(token_batch)\n",
    "    b = len(prompts)\n",
    "    img = torch.randn(b, 1, 32, 32, device=device)\n",
    "    for i in tqdm(reversed(range(T)), desc=\"sampling\"):\n",
    "        t = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "        t_emb = get_time_embedding(t)\n",
    "        img, _ = p_sample(model_to_use, img, t, t_emb, text_emb)\n",
    "    img = img.clamp(-1, 1)\n",
    "    return img\n",
    "\n",
    "example_prompts = [\n",
    "    \"a hand-drawn circle\",\n",
    "    \"a hand-drawn triangle\",\n",
    "    \"a hand-drawn square\",\n",
    "]\n",
    "\n",
    "samples = sample(example_prompts)\n",
    "grid = make_grid(samples, nrow=3, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8856955,
     "sourceId": 13901761,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
