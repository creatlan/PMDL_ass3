{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shape-Conditioned Diffusion (text-to-image)",
        "",
        "This notebook trains a small text-conditioned diffusion model on the hand-drawn circles, triangles, and squares dataset. It includes data sanity checks, a cosine noise schedule, an EMA-stabilized U-Net, and a deterministic sampling loop so epochs and outputs stay consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install (if needed)",
        "Run this once if your environment does not already provide the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If you already have torch/torchvision/numpy/matplotlib installed you can skip this cell.",
        "# The `-q` flag keeps output compact.",
        "# !pip install -q torch torchvision torchaudio matplotlib tqdm numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and reproducibility",
        "Set seeds and pick the GPU automatically when available. The same seeds are reused in sampling for repeatability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.auto import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading and prep",
        "The dataset ships as `data/circle.npy`, `data/triangle.npy`, and `data/square.npy`. Each array contains grayscale drawings. To keep epochs bounded around ~6k samples, the loader optionally caps each class. Light geometric jittering helps the tiny model generalize and prevents collapse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class ShapeTextDataset(Dataset):\n",
        "    def __init__(self, data_dir: str = \"data\", img_size: int = 32, max_items_per_class: int = 1000, augment: bool = True):\n",
        "        self.samples: List[Tuple[np.ndarray, str]] = []\n",
        "        prompt_map = {\n",
        "            \"circle\": \"a hand-drawn black circle on white\",\n",
        "            \"square\": \"a hand-drawn black square on white\",\n",
        "            \"triangle\": \"a hand-drawn black triangle on white\",\n",
        "        }\n",
        "        for name in [\"circle\", \"square\", \"triangle\"]:\n",
        "            path = os.path.join(data_dir, f\"{name}.npy\")\n",
        "            arr = np.load(path, mmap_mode=\"r\")\n",
        "            arr = arr[:max_items_per_class]\n",
        "            if arr.ndim == 4:  # (N,H,W,C)\n",
        "                arr = arr[..., 0]\n",
        "            self.samples.extend([(img.astype(np.float32), prompt_map[name]) for img in arr])\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _prepare_tensor(self, img: np.ndarray) -> torch.Tensor:\n",
        "        tensor = torch.as_tensor(img, dtype=torch.float32)\n",
        "        if tensor.ndim == 1:\n",
        "            side = int(math.sqrt(tensor.numel()))\n",
        "            tensor = tensor.view(side, side)\n",
        "        if tensor.ndim == 2:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        if tensor.shape[0] > 1:\n",
        "            tensor = tensor.mean(dim=0, keepdim=True)\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, prompt = self.samples[idx]\n",
        "        img = self._prepare_tensor(img)\n",
        "        img = TF.resize(img, [self.img_size, self.img_size])\n",
        "        if self.augment:\n",
        "            angle = random.uniform(-8, 8)\n",
        "            img = TF.rotate(img, angle, fill=0.0)\n",
        "            img = TF.affine(img, angle=0.0, translate=(random.uniform(-2, 2), random.uniform(-2, 2)), scale=1.0, shear=0.0, fill=0.0)\n",
        "        img = (img / 255.0).clamp(0, 1) * 2 - 1\n",
        "        return img, prompt\n",
        "\n",
        "\n",
        "def get_dataloader(batch_size=32, img_size=32, max_items_per_class=1000, augment=True, num_workers=4):\n",
        "    dataset = ShapeTextDataset(img_size=img_size, max_items_per_class=max_items_per_class, augment=augment)\n",
        "    print(f\"Loaded {len(dataset)} total sketches (cap {max_items_per_class} per class)\")\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=bool(num_workers),\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        prefetch_factor=2 if num_workers else None,\n",
        "    )\n",
        "\n",
        "\n",
        "# Default training loader; if the first batch stalls, temporarily lower num_workers\n",
        "dataloader = get_dataloader(batch_size=16, img_size=32, max_items_per_class=1000, augment=True, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick look at data",
        "Confirm that the images align with the prompts after preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick sanity-check batch\n",
        "If the preview below ever hangs for more than a minute, drop `num_workers` to 0. \n",
        "The cap of 64 sketches per class and `mmap` loading keeps the first batch fast (a few seconds).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "preview_loader = get_dataloader(batch_size=6, img_size=32, max_items_per_class=64, augment=False, num_workers=0)\n",
        "imgs, prompts = next(iter(preview_loader))\n",
        "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
        "for i in range(6):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(((imgs[i].squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
        "    ax.set_title(prompts[i][:10] + \"...\")\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diffusion helpers",
        "Cosine noise schedule with sinusoidal time embeddings. The helper functions implement the forward process and sampling utilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "T = 200\n",
        "betas = cosine_beta_schedule(T)\n",
        "alphas = 1.0 - betas\n",
        "alpha_cum = torch.cumprod(alphas, dim=0)\n",
        "alpha_cum_prev = torch.cat([torch.ones(1), alpha_cum[:-1]])\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t.cpu()).float().to(t.device)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    return extract(alpha_cum.sqrt(), t, x_start.shape) * x_start + extract((1 - alpha_cum).sqrt(), t, x_start.shape) * noise\n",
        "\n",
        "\n",
        "def p_mean_variance(model, x, t, t_emb, text_emb):\n",
        "    pred_noise = model(x, t_emb, text_emb)\n",
        "    beta_t = extract(betas, t, x.shape)\n",
        "    alpha_t = extract(alphas, t, x.shape)\n",
        "    alpha_cum_t = extract(alpha_cum, t, x.shape)\n",
        "    coef1 = 1 / torch.sqrt(alpha_t)\n",
        "    coef2 = beta_t / torch.sqrt(1 - alpha_cum_t)\n",
        "    mean = coef1 * (x - coef2 * pred_noise)\n",
        "    var = beta_t\n",
        "    return mean, var, pred_noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_emb, text_emb):\n",
        "    mean, var, pred_noise = p_mean_variance(model, x, t, t_emb, text_emb)\n",
        "    if (t == 0).all():\n",
        "        return mean, pred_noise\n",
        "    noise = torch.randn_like(x)\n",
        "    return mean + torch.sqrt(var) * noise, pred_noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text encoder and tiny U-Net",
        "GRU-based text encoder feeds a light U-Net. Time and text embeddings are injected into each block to keep conditioning stable. An EMA copy of the network is maintained for sharper sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=128, hidden_dim=192):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(1024, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        emb = self.embedding(tokens)\n",
        "        _, h = self.rnn(emb)\n",
        "        return self.proj(h.squeeze(0))\n",
        "\n",
        "\n",
        "def tokenize(prompts: List[str], vocab=None, max_len=16):\n",
        "    if vocab is None:\n",
        "        vocab = {}\n",
        "    token_lists = []\n",
        "    for text in prompts:\n",
        "        tokens = []\n",
        "        for word in text.lower().split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab) + 1\n",
        "            tokens.append(vocab[word])\n",
        "        token_lists.append(tokens[:max_len])\n",
        "    max_len = max(len(t) for t in token_lists)\n",
        "    padded = []\n",
        "    for t in token_lists:\n",
        "        padded.append(t + [0] * (max_len - len(t)))\n",
        "    return torch.tensor(padded, dtype=torch.long), vocab\n",
        "\n",
        "\n",
        "def sinusoidal_embedding(n, d):\n",
        "    pos = torch.arange(n)[:, None]\n",
        "    dim = torch.arange(d)[None, :]\n",
        "    angle = pos / (10000 ** (2 * (dim // 2) / d))\n",
        "    emb = torch.zeros((n, d))\n",
        "    emb[:, 0::2] = torch.sin(angle[:, 0::2])\n",
        "    emb[:, 1::2] = torch.cos(angle[:, 1::2])\n",
        "    return emb\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class TinyUNet(nn.Module):\n",
        "    def __init__(self, base=24, time_dim=128, text_dim=192):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_dim, time_dim), nn.SiLU())\n",
        "        self.text_mlp = nn.Sequential(nn.Linear(text_dim, text_dim), nn.SiLU())\n",
        "        cond_channels = {\"d1\": base, \"d2\": base * 2, \"mid\": base * 4, \"u1\": base * 2, \"u2\": base}\n",
        "        self.time_proj = nn.ModuleDict({k: nn.Linear(time_dim, v) for k, v in cond_channels.items()})\n",
        "        self.text_proj = nn.ModuleDict({k: nn.Linear(text_dim, v) for k, v in cond_channels.items()})\n",
        "        self.down1 = ConvBlock(1, base)\n",
        "        self.down2 = ConvBlock(base, base * 2)\n",
        "        self.to_vec = ConvBlock(base * 2, base * 4)\n",
        "        self.up1 = ConvBlock(base * 4, base * 2)\n",
        "        self.up2 = ConvBlock(base * 2, base)\n",
        "        self.skip_proj = nn.Conv2d(base, base * 2, 1)\n",
        "        self.out = nn.Conv2d(base, 1, 1)\n",
        "\n",
        "    def _inject(self, feat, name, t_feat, txt_feat):\n",
        "        t = self.time_proj[name](t_feat)[:, :, None, None]\n",
        "        txt = self.text_proj[name](txt_feat)[:, :, None, None]\n",
        "        return feat + t + txt\n",
        "\n",
        "    def forward(self, x, t_embed, text_embed):\n",
        "        t_feat = self.time_mlp(t_embed)\n",
        "        txt_feat = self.text_mlp(text_embed)\n",
        "        d1 = self.down1(x)\n",
        "        d1 = self._inject(d1, \"d1\", t_feat, txt_feat)\n",
        "        d2 = self.down2(nn.functional.avg_pool2d(d1, 2))\n",
        "        d2 = self._inject(d2, \"d2\", t_feat, txt_feat)\n",
        "        mid = self.to_vec(d2)\n",
        "        mid = self._inject(mid, \"mid\", t_feat, txt_feat)\n",
        "        u1 = nn.functional.interpolate(self.up1(mid), scale_factor=2, mode=\"nearest\")\n",
        "        skip = self.skip_proj(d1)\n",
        "        u1 = self._inject(u1 + skip, \"u1\", t_feat, txt_feat)\n",
        "        u2 = self.up2(u1)\n",
        "        u2 = self._inject(u2, \"u2\", t_feat, txt_feat)\n",
        "        return self.out(u2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop",
        "The model predicts the noise added at timestep *t*. EMA smoothing, gradient clipping, and a cosine learning rate schedule stabilize training. Default settings keep each epoch to the ~6k sketches cap; increase `max_items_per_class` to use the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "text_encoder = TextEncoder().to(device)\n",
        "model = TinyUNet().to(device)\n",
        "ema_model = TinyUNet().to(device)\n",
        "ema_model.load_state_dict(model.state_dict())\n",
        "ema_decay = 0.995\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(text_encoder.parameters()), lr=2e-4)\n",
        "scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "vocab: Dict[str, int] = {}\n",
        "\n",
        "\n",
        "def get_time_embedding(timesteps, dim=128):\n",
        "    emb = sinusoidal_embedding(max(T, timesteps.max().item() + 1), dim).to(device)\n",
        "    return emb[timesteps]\n",
        "\n",
        "\n",
        "def update_ema(model_src, model_tgt, decay):\n",
        "    with torch.no_grad():\n",
        "        for p_src, p_tgt in zip(model_src.parameters(), model_tgt.parameters()):\n",
        "            p_tgt.mul_(decay).add_(p_src, alpha=1 - decay)\n",
        "\n",
        "\n",
        "def p_losses(x0, prompts):\n",
        "    b = x0.shape[0]\n",
        "    t = torch.randint(0, T, (b,), device=device).long()\n",
        "    noise = torch.randn_like(x0)\n",
        "    x_noisy = q_sample(x0, t, noise)\n",
        "    global vocab\n",
        "    token_batch, vocab = tokenize(list(prompts), vocab)\n",
        "    token_batch = token_batch.to(device)\n",
        "    text_emb = text_encoder(token_batch)\n",
        "    t_emb = get_time_embedding(t)\n",
        "    pred = model(x_noisy, t_emb, text_emb)\n",
        "    return nn.functional.mse_loss(pred, noise)\n",
        "\n",
        "\n",
        "def train(epochs=5, grad_clip=1.0, checkpoint_dir=\"checkpoints\"):\n",
        "    global ema_model\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=epochs * len(dataloader), eta_min=5e-5\n",
        "    )\n",
        "\n",
        "    epoch_bar = tqdm(range(epochs), desc=\"training epochs\")\n",
        "    for epoch in epoch_bar:\n",
        "        loop = tqdm(dataloader, desc=f\"epoch {epoch + 1}\", leave=False)\n",
        "        running_loss = 0.0\n",
        "        for step, (imgs, prompts) in enumerate(loop, 1):\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\", dtype=torch.float16 if torch.cuda.is_available() else torch.bfloat16):\n",
        "                loss = p_losses(imgs, prompts)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(text_encoder.parameters()), grad_clip)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            update_ema(model, ema_model, ema_decay)\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item(), avg_loss=running_loss / step)\n",
        "\n",
        "        checkpoint = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"ema\": ema_model.state_dict(),\n",
        "            \"text\": text_encoder.state_dict(),\n",
        "            \"vocab\": vocab,\n",
        "        }\n",
        "        latest_path = os.path.join(checkpoint_dir, \"shape_diffusion_latest.pt\")\n",
        "        epoch_path = os.path.join(checkpoint_dir, f\"shape_diffusion_epoch{epoch + 1}.pt\")\n",
        "        torch.save(checkpoint, latest_path)\n",
        "        torch.save(checkpoint, epoch_path)\n",
        "        epoch_bar.set_postfix(last_loss=loss.item())\n",
        "        print(f\"Epoch {epoch + 1} checkpoints saved -> {epoch_path}\")\n",
        "\n",
        "\n",
        "# Kick off training (adjust epochs upward for higher fidelity once you are satisfied with runtime)\n",
        "# train(epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling from the model",
        "Start from random noise and apply the reverse diffusion steps conditioned on a prompt. EMA weights are preferred; if training is skipped, fall back to the current model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()",
        "def sample(prompts: List[str], use_ema: bool = True):",
        "    model_to_use = ema_model if use_ema else model",
        "    model_to_use.eval()",
        "    global vocab",
        "    token_batch, vocab = tokenize(prompts, vocab)",
        "    token_batch = token_batch.to(device)",
        "    text_emb = text_encoder(token_batch)",
        "    b = len(prompts)",
        "    img = torch.randn(b, 1, 32, 32, device=device)",
        "    for i in tqdm(reversed(range(T)), desc=\"sampling\"):",
        "        t = torch.full((b,), i, device=device, dtype=torch.long)",
        "        t_emb = get_time_embedding(t)",
        "        img, _ = p_sample(model_to_use, img, t, t_emb, text_emb)",
        "    img = img.clamp(-1, 1)",
        "    return img",
        "",
        "example_prompts = [",
        "    \"a hand-drawn circle\",",
        "    \"a hand-drawn triangle\",",
        "    \"a hand-drawn square\",",
        "]",
        "",
        "# Uncomment after training",
        "# samples = sample(example_prompts)",
        "# grid = make_grid(samples, nrow=3, normalize=True, value_range=(-1, 1))",
        "# plt.figure(figsize=(9, 3))",
        "# plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap=\"gray\")",
        "# plt.axis(\"off\")",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}