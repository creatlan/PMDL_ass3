{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PMLDL Assignment 3: Diffusion\n",
        "\n",
        "## Option 1: Text-to-Image Generation\n",
        "Text-to-Image Generation is a type of image generation process where the model creates images based on specific input textual prompts. These prompts guide the output, allowing control over the visual characteristics or content of the generated images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Imports\n",
        "install the basic libraries and bring the imports I need for tensors, plotting, and loading the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If your runtime misses these libraries, run this cell once.\n",
        "# In many university labs this is enough to get numpy, torch, and plotting tools.\n",
        "!pip install -q numpy torch torchvision matplotlib tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading and prep\n",
        "I load the tiny shape handwritten dataset (circle, square, triangle) and pair each image with a simple text prompt. By resizing and normalizing these photos to a small square training stays fast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 371191\n"
          ]
        }
      ],
      "source": [
        "class ShapeTextDataset(Dataset):\n",
        "    def __init__(self, data_dir=\"data\", img_size=32):\n",
        "        self.samples = []\n",
        "        prompt_map = {\n",
        "            \"circle\": \"a simple black circle on white\",\n",
        "            \"square\": \"a simple black square on white\",\n",
        "            \"triangle\": \"a simple black triangle on white\",\n",
        "        }\n",
        "        for name in [\"circle\", \"square\", \"triangle\"]:\n",
        "            imgs = np.load(f\"{data_dir}/{name}.npy\")  # expected shape (N,H,W) or flattened\n",
        "            if imgs.ndim == 4:\n",
        "                imgs = imgs[..., 0]\n",
        "            self.samples.extend([(img.astype(np.float32), prompt_map[name]) for img in imgs])\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _prepare_tensor(self, img: np.ndarray) -> torch.Tensor:\n",
        "        tensor = torch.as_tensor(img, dtype=torch.float32)\n",
        "        if tensor.ndim == 0:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        if tensor.ndim == 1:  # reshape to square\n",
        "            side = int(math.sqrt(tensor.numel()))\n",
        "            if side * side != tensor.numel():\n",
        "                raise ValueError(\"Flattened image cannot form a square\")\n",
        "            tensor = tensor.view(side, side)\n",
        "        if tensor.ndim == 3:\n",
        "            if tensor.shape[0] in (1, 3):\n",
        "                pass\n",
        "            elif tensor.shape[-1] in (1, 3):\n",
        "                tensor = tensor.permute(2, 0, 1)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported 3D tensor shape for image data\")\n",
        "        if tensor.ndim == 2:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        if tensor.shape[0] > 1:  # single channel for diffusion model\n",
        "            tensor = tensor.mean(dim=0, keepdim=True)\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, prompt = self.samples[idx]\n",
        "        img = self._prepare_tensor(img)\n",
        "        img = torch.nn.functional.interpolate(\n",
        "            img.unsqueeze(0), size=(self.img_size, self.img_size), mode=\"bilinear\", align_corners=False\n",
        "        ).squeeze(0)\n",
        "        img = (img / 255.0).clamp(0, 1) * 2 - 1  # scale to [-1,1]\n",
        "        return img, prompt\n",
        "\n",
        "\n",
        "def get_dataloader(batch_size=32, img_size=32):\n",
        "    dataset = ShapeTextDataset(img_size=img_size)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "dataloader = get_dataloader(batch_size=16, img_size=32)\n",
        "print(\"Total samples:\", len(dataloader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick look at data\n",
        "Plot of a few samples to make sure the loader works and the prompts line up with the sketches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x200 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "imgs, prompts = next(iter(dataloader))\n",
        "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
        "for i in range(6):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(((imgs[i].squeeze() + 1) / 2).clamp(0,1), cmap=\"gray\")\n",
        "    ax.set_title(prompts[i][:10] + \"...\")\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diffusion helpers\n",
        "I set up the beta schedule, sinusoidal time embeddings, and helper functions for the forward diffusion process.\n",
        "\n",
        "By baking in the cosine beta schedule and sinusoidal embeddings we will get smoother variance transitions, which stabilizes training and makes the reverse sampler behave predictably once the U-Net learns to denoise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "T = 200\n",
        "betas = cosine_beta_schedule(T)\n",
        "alphas = 1.0 - betas\n",
        "alpha_cum = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t.cpu()).float().to(t.device)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    sqrt_ac = torch.sqrt(extract(alpha_cum, t, x_start.shape))\n",
        "    sqrt_om = torch.sqrt(1 - extract(alpha_cum, t, x_start.shape))\n",
        "    return sqrt_ac * x_start + sqrt_om * noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text encoder and tiny U-Net\n",
        "I encode the text with a GRU into a single vector and feed that into a light U-Net. The text and time embeddings modulate the conv blocks.\n",
        "\n",
        "This pairing lets the textual meaning flow into every spatial feature map via per-block modulation, so each denoising step “knows” which shape to emphasize. Keeping the UNet tiny while conditioning through these learned projections kept training fast yet still expressive enough for the three-shape vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=128, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(1000, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        emb = self.embedding(tokens)\n",
        "        _, h = self.rnn(emb)\n",
        "        return self.proj(h.squeeze(0))\n",
        "\n",
        "\n",
        "def tokenize(prompts: List[str], vocab=None, max_len=16):\n",
        "    if vocab is None:\n",
        "        vocab = {}\n",
        "    token_lists = []\n",
        "    for text in prompts:\n",
        "        tokens = []\n",
        "        for word in text.lower().split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab) + 1\n",
        "            tokens.append(vocab[word])\n",
        "        token_lists.append(tokens[:max_len])\n",
        "    max_len = max(len(t) for t in token_lists)\n",
        "    padded = []\n",
        "    for t in token_lists:\n",
        "        padded.append(t + [0] * (max_len - len(t)))\n",
        "    return torch.tensor(padded, dtype=torch.long), vocab\n",
        "\n",
        "\n",
        "def sinusoidal_embedding(n, d):\n",
        "    pos = torch.arange(n)[:, None]\n",
        "    dim = torch.arange(d)[None, :]\n",
        "    angle = pos / (10000 ** (2 * (dim // 2) / d))\n",
        "    emb = torch.zeros((n, d))\n",
        "    emb[:, 0::2] = torch.sin(angle[:, 0::2])\n",
        "    emb[:, 1::2] = torch.cos(angle[:, 1::2])\n",
        "    return emb\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class TinyUNet(nn.Module):\n",
        "    def __init__(self, base=32, time_dim=128, text_dim=128):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_dim, time_dim), nn.SiLU())\n",
        "        self.text_mlp = nn.Sequential(nn.Linear(text_dim, text_dim), nn.SiLU())\n",
        "        cond_channels = {\n",
        "            \"d1\": base,\n",
        "            \"d2\": base * 2,\n",
        "            \"mid\": base * 4,\n",
        "            \"u1\": base * 2,\n",
        "            \"u2\": base,\n",
        "        }\n",
        "        self.time_proj = nn.ModuleDict({k: nn.Linear(time_dim, v) for k, v in cond_channels.items()})\n",
        "        self.text_proj = nn.ModuleDict({k: nn.Linear(text_dim, v) for k, v in cond_channels.items()})\n",
        "\n",
        "        self.down1 = ConvBlock(1, base)\n",
        "        self.down2 = ConvBlock(base, base * 2)\n",
        "        self.to_vec = nn.Conv2d(base * 2, base * 4, 3, padding=1)\n",
        "        self.up1 = ConvBlock(base * 4, base * 2)\n",
        "        self.up2 = ConvBlock(base * 2, base)\n",
        "        self.skip_proj = nn.Conv2d(base, base * 2, 1)\n",
        "        self.out = nn.Conv2d(base, 1, 1)\n",
        "\n",
        "    def _inject(self, feat, name, t_feat, txt_feat):\n",
        "        t = self.time_proj[name](t_feat)[:, :, None, None]\n",
        "        txt = self.text_proj[name](txt_feat)[:, :, None, None]\n",
        "        return feat + t + txt\n",
        "\n",
        "    def forward(self, x, t_embed, text_embed):\n",
        "        t_feat = self.time_mlp(t_embed)\n",
        "        txt_feat = self.text_mlp(text_embed)\n",
        "\n",
        "        d1 = self.down1(x)\n",
        "        d1 = self._inject(d1, \"d1\", t_feat, txt_feat)\n",
        "        d2 = self.down2(nn.functional.avg_pool2d(d1, 2))\n",
        "        d2 = self._inject(d2, \"d2\", t_feat, txt_feat)\n",
        "        mid = self.to_vec(d2)\n",
        "        mid = self._inject(mid, \"mid\", t_feat, txt_feat)\n",
        "        u1 = nn.functional.interpolate(self.up1(mid), scale_factor=2, mode=\"nearest\")\n",
        "        skip = self.skip_proj(d1)\n",
        "        u1 = self._inject(u1 + skip, \"u1\", t_feat, txt_feat)\n",
        "        u2 = self.up2(u1)\n",
        "        u2 = self._inject(u2, \"u2\", t_feat, txt_feat)\n",
        "        return self.out(u2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "I pick a small number of steps so it runs on a laptop. The loss trains the U-Net to predict the noise added at each timestep.\n",
        "\n",
        "Approximate time for training ~1.25h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b61e11178e684795aa78efd2e3452cb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "epoch 1:   0%|          | 0/23200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "text_encoder = TextEncoder().to(device)\n",
        "model = TinyUNet().to(device)\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(text_encoder.parameters()), lr=1e-3)\n",
        "vocab = {}\n",
        "\n",
        "\n",
        "def get_time_embedding(timesteps, dim=128):\n",
        "    emb = sinusoidal_embedding(max(T, timesteps.max().item() + 1), dim).to(device)\n",
        "    return emb[timesteps]\n",
        "\n",
        "\n",
        "def p_losses(x0, prompts):\n",
        "    b = x0.shape[0]\n",
        "    t = torch.randint(0, T, (b,), device=device).long()\n",
        "    noise = torch.randn_like(x0)\n",
        "    x_noisy = q_sample(x0, t, noise)\n",
        "    global vocab\n",
        "    token_batch, vocab = tokenize(list(prompts), vocab)\n",
        "    token_batch = token_batch.to(device)\n",
        "    text_emb = text_encoder(token_batch)\n",
        "    t_emb = get_time_embedding(t)\n",
        "    pred = model(x_noisy, t_emb, text_emb)\n",
        "    return nn.functional.mse_loss(pred, noise)\n",
        "\n",
        "\n",
        "def train(epochs=3):\n",
        "    model.train()\n",
        "    text_encoder.train()\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(dataloader, desc=f\"epoch {epoch+1}\")\n",
        "        for imgs, prompts in loop:\n",
        "            imgs = imgs.to(device)\n",
        "            loss = p_losses(imgs, prompts)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "train(epochs=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling from the model\n",
        "I start from random noise and apply the reverse diffusion steps while conditioning on the text prompt. A few prompts show the link between words and shapes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, text_emb):\n",
        "    betas_t = extract(betas, t, x.shape).to(device)\n",
        "    sqrt_one_minus_ac = torch.sqrt(1 - extract(alpha_cum, t, x.shape)).to(device)\n",
        "    sqrt_recip_alpha = torch.sqrt(1.0 / extract(alphas, t, x.shape)).to(device)\n",
        "\n",
        "    model_mean = sqrt_recip_alpha * (x - betas_t * model(x, get_time_embedding(t), text_emb) / sqrt_one_minus_ac)\n",
        "    if t.item() == 0:\n",
        "        return model_mean\n",
        "    noise = torch.randn_like(x)\n",
        "    return model_mean + torch.sqrt(betas_t) * noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(prompts: List[str], img_size=32):\n",
        "    model.eval(); text_encoder.eval()\n",
        "    token_batch, _ = tokenize(prompts, vocab)\n",
        "    token_batch = token_batch.to(device)\n",
        "    text_emb = text_encoder(token_batch)\n",
        "    b = len(prompts)\n",
        "    img = torch.randn((b, 1, img_size, img_size), device=device)\n",
        "    for i in reversed(range(T)):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), text_emb)\n",
        "    img = (img.clamp(-1, 1) + 1) / 2\n",
        "    return img.cpu()\n",
        "\n",
        "example_prompts = [\n",
        "    \"a simple black circle on white\",\n",
        "    \"a simple black square on white\",\n",
        "    \"a simple black triangle on white\",\n",
        "]\n",
        "\n",
        "samples = sample(example_prompts)\n",
        "grid = make_grid(samples, nrow=len(example_prompts), normalize=False)\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.imshow(grid.squeeze(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
