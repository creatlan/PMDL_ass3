{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Shape-Conditioned Diffusion (text-to-image)\n",
        "\n",
        "This notebook trains a small text-conditioned diffusion model on the hand-drawn circles, triangles, and squares dataset. It includes data sanity checks, a cosine noise schedule, an EMA-stabilized U-Net, and a deterministic sampling loop so epochs and outputs stay consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install (if needed)\n",
        "Run this once if your environment does not already provide the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100dbefe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you already have torch/torchvision/numpy/matplotlib installed you can skip this cell.\n",
        "# The `-q` flag keeps output compact.\n",
        "# !pip install -q torch torchvision torchaudio matplotlib tqdm numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61eef19b",
      "metadata": {},
      "source": [
        "## Imports and reproducibility\n",
        "Set seeds and pick the GPU automatically when available. The same seeds are reused in sampling for repeatability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9eabe82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ae105e",
      "metadata": {},
      "source": [
        "## Data loading and prep\n",
        "The dataset ships as `data/circle.npy`, `data/triangle.npy`, and `data/square.npy`. Each array contains grayscale drawings. To keep epochs bounded around ~6k samples, the loader optionally caps each class. Light geometric jittering helps the tiny model generalize and prevents collapse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71396315",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 6000 total sketches (cap 2000 per class)\n"
          ]
        }
      ],
      "source": [
        "class ShapeTextDataset(Dataset):\n",
        "    def __init__(self, data_dir: str = \"data\", img_size: int = 32, max_items_per_class: int = 2000, augment: bool = True):\n",
        "        self.samples: List[Tuple[np.ndarray, str]] = []\n",
        "        prompt_map = {\n",
        "            \"circle\": \"a hand-drawn white circle on black\",\n",
        "            \"square\": \"a hand-drawn white square on black\",\n",
        "            \"triangle\": \"a hand-drawn white triangle on black\",\n",
        "        }\n",
        "        for name in [\"circle\", \"square\", \"triangle\"]:\n",
        "            path = os.path.join(data_dir, f\"{name}.npy\")\n",
        "            # mmap keeps load time bounded even if the raw files contain far more than the capped subset\n",
        "            arr = np.load(path, mmap_mode=\"r\")\n",
        "            arr = arr[:max_items_per_class]\n",
        "            if arr.ndim == 4:  # (N,H,W,C)\n",
        "                arr = arr[..., 0]\n",
        "            self.samples.extend([(img.astype(np.float32), prompt_map[name]) for img in arr])\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _prepare_tensor(self, img: np.ndarray) -> torch.Tensor:\n",
        "        tensor = torch.as_tensor(img, dtype=torch.float32)\n",
        "        if tensor.ndim == 1:\n",
        "            side = int(math.sqrt(tensor.numel()))\n",
        "            tensor = tensor.view(side, side)\n",
        "        if tensor.ndim == 2:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        if tensor.shape[0] > 1:\n",
        "            tensor = tensor.mean(dim=0, keepdim=True)\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, prompt = self.samples[idx]\n",
        "        img = self._prepare_tensor(img)\n",
        "        img = TF.resize(img, [self.img_size, self.img_size])\n",
        "        if self.augment:\n",
        "            angle = random.uniform(-8, 8)\n",
        "            img = TF.rotate(img, angle, fill=0.0)\n",
        "            img = TF.affine(img, angle=0.0, translate=(random.uniform(-2, 2), random.uniform(-2, 2)), scale=1.0, shear=0.0, fill=0.0)\n",
        "        img = (img / 255.0).clamp(0, 1) * 2 - 1\n",
        "        return img, prompt\n",
        "\n",
        "\n",
        "def get_dataloader(batch_size=32, img_size=32, max_items_per_class=2000, augment=True, num_workers=2):\n",
        "    dataset = ShapeTextDataset(img_size=img_size, max_items_per_class=max_items_per_class, augment=augment)\n",
        "    print(f\"Loaded {len(dataset)} total sketches (cap {max_items_per_class} per class)\")\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        persistent_workers=bool(num_workers),\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "\n",
        "# Default training loader; if the first batch stalls, temporarily lower num_workers\n",
        "dataloader = get_dataloader(batch_size=16, img_size=32, max_items_per_class=2000, augment=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f42b5b51",
      "metadata": {},
      "source": [
        "### Quick look at data\n",
        "Confirm that the images align with the prompts after preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3f4ce3",
      "metadata": {},
      "source": [
        "### Quick sanity-check batch\n",
        "If the preview below ever hangs for more than a minute, drop `num_workers` to 0. \n",
        "The cap of 64 sketches per class and `mmap` loading keeps the first batch fast (a few seconds).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef85ba16",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 192 total sketches (cap 64 per class)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAADICAYAAABlEhH4AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAALv9JREFUeJzt3Xl0VeW9//EnQEjClEAIYYhMAYIBIcogoCgKCBVEBlFEr0aWQ9tVra0Xe733itp2ldZVr1OxvS4rxaJ4QQEFBAOCoowiSiRhDEmYEoZMJCQhCeT3x2+V5cnnC+6EhBxy3q+1ulbPpzsnT87Zz977PD18dlBlZWWlAwAAAAAAAH6gUX0PAAAAAAAAAP6HRSMAAAAAAAAIFo0AAAAAAAAgWDQCAAAAAACAYNEIAAAAAAAAgkUjAAAAAAAACBaNAAAAAAAAIFg0AgAAAAAAgGDRCAAAAAAAAKLBLRplZGS4oKAg9+c//7m+h+IjKCjIPf/88zX++cTERNe1a9daGw8CC/MC8MWcABTzAvDFnAAU8yLwNLhFIwAAAAAAAFw6Fo0AAAAAAAAgWDRqQCoqKlxZWVl9DwPwK8wLwBdzAlDMC8AXcwJQgTov/HLRKDMz0/385z93cXFxLiwszEVGRrqpU6e6jIyMaj3Pm2++6WJjY11ISIgbNGiQ+/rrr33+9+TkZJeYmOi6d+/uQkNDXfv27d2MGTNcTk6Oz3bPP/+8CwoKcvv373eJiYkuIiLChYeHu4ceesgVFxf7bHvmzBn3q1/9ykVFRbmWLVu6CRMmuMOHD1dr3EuXLnV9+/Z1oaGhrm/fvm7JkiWyzQ//Lekrr7xy/u9MTU11ZWVlbtasWW7AgAEuPDzcNW/e3A0fPtytW7euWuOAf2FeMC/giznBnIBiXjAv4Is5wZyAYl4wL6qjSX0PwPL111+7jRs3umnTprmYmBiXkZHh/vrXv7oRI0a41NRU16xZsx99jvfee88VFha6xx57zAUFBbkXX3zRTZ482R04cMAFBwc755xbvXq1O3DggHvooYdc+/btXUpKinvzzTddSkqK27x5swsKCvJ5zrvvvtt169bNzZ49223fvt299dZbrl27du5Pf/rT+W0efvhhN3/+fDd9+nQ3bNgwt3btWjdu3DjPf3tSUpKbMmWKi4+Pd7Nnz3Y5OTnuoYcecjExMeb2c+fOdaWlpe7RRx91ISEhrk2bNu7UqVPurbfecvfee6975JFHXGFhofv73//uxowZ47Zu3eoSEhI8jwf+g3nBvIAv5gRzAop5wbyAL+YEcwKKecG8qJZKP1RcXCzZpk2bKp1zle+8885FfzY9Pb3SOVcZGRlZmZubez7/6KOPKp1zlcuWLbvo71mwYEGlc65y/fr157Pnnnuu0jlXOWPGDJ9tJ02aVBkZGXn+8XfffVfpnKv8+c9/7rPd9OnTK51zlc8999xFx15ZWVmZkJBQ2aFDh8r8/PzzWVJSUqVzrrJLly7yd7Zq1ary+PHjPs9RUVFReebMGZ8sLy+vMjo6Wv4GXDmYF8wL+GJOMCegmBfMC/hiTjAnoJgXzIvq8Mt/nhYWFnb+v5eXl7ucnBzXo0cPFxER4bZv3+7pOe655x7XunXr84+HDx/unHPuwIED5u8pLS11J0+edEOGDHHOOfP3/PSnP/V5PHz4cJeTk+NOnTrlnHPuk08+cc4598QTT/hs9+STT3oac1ZWlvvuu+/cgw8+6MLDw8/no0ePdvHx8ebPTJkyxUVFRflkjRs3dk2bNnXOOXfu3DmXm5vrKioq3MCBAz2/fvA/zAvmBXwxJ5gTUMwL5gV8MSeYE1DMC+ZFdfjlolFJSYmbNWuWu+qqq1xISIhr27ati4qKcvn5+a6goMDTc3Tu3Nnn8b926Ly8vPNZbm6u++Uvf+mio6NdWFiYi4qKct26dXPOOfP3/NhzZmZmukaNGrnY2Fif7eLi4nwel5WVuezsbJ//nD171mVmZjrnnOvZs6f87qrP8S//Gm9V8+bNc/369XOhoaEuMjLSRUVFuRUrVnh+/eB/mBfMC/hiTjAnoJgXzAv4Yk4wJ6CYF8yL6vDLTqPHH3/czZ071z355JNu6NChLjw83AUFBblp06a5c+fOeXqOxo0bm3llZeX5/3733Xe7jRs3upkzZ7qEhATXokULd+7cOTd27Fjz93h5Ti82btzobrnlFp8sPT29Ws/xLz9cvf2X+fPnu8TERDdx4kQ3c+ZM165dO9e4cWM3e/Zsl5aWVqPfg/rHvPCOeREYmBPeMScCB/PCO+ZFYGBOeMecCBzMC++YF366aPTBBx+4Bx980L300kvns9LSUpefn19rvyMvL8999tln7oUXXnCzZs06n+/bt6/Gz9mlSxd37tw5l5aW5rNSuWfPHp/t+vfv71avXu2TtW/f3oWEhFxwDFWf42I++OAD1717d7d48WKfcrHnnnvO83PA/zAvmBfwxZxgTkAxL5gX8MWcYE5AMS+YF9Xhl/88rXHjxrKa+Prrr7uzZ8/W6u9wTlctX3nllRo/509+8hPnnHOvvfbaRZ+zdevWbtSoUT7/CQ0NdR06dHAJCQlu3rx5Pl9rW716tUtNTfU8Dutv27Jli9u0aZNsm5aWJquhWVlZbvfu3a68vPx8VlBQ4Hbv3t0gv253pWBeMC/giznBnIBiXjAv4Is5wZyAYl4wL6rDL79pNH78ePfPf/7ThYeHu/j4eLdp0ya3Zs0aFxkZWWu/o1WrVu6mm25yL774oisvL3edOnVySUlJNf7amnPOJSQkuHvvvde98cYbrqCgwA0bNsx99tlnbv/+/Z6fY/bs2W7cuHHuxhtvdDNmzHC5ubnu9ddfd3369HFFRUWenmP8+PFu8eLFbtKkSW7cuHEuPT3d/e1vf3Px8fHyHCNHjnTOOZeRkXE+e+aZZ9y8efNcenq669q1q3POuSVLlriHHnrIzZ071yUmJnr+e1B7mBfMC/hiTjAnoJgXzAv4Yk4wJ6CYF8yL6vDLbxq9+uqr7oEHHnDvvvuue+qpp1xWVpZbs2aNa9GiRa3+nvfee8+NGTPGzZkzxz3zzDMuODjYrVy58pKe8+2333ZPPPGEW7VqlXv66addeXm5W7FiheefHzt2rFu0aJE7e/ase+aZZ9zixYvd3Llz3cCBAz0/R2JiovvDH/7gduzY4Z544gn36aefuvnz51frOeB/mBfMC/hiTjAnoJgXzAv4Yk4wJ6CYF8yL6giqrG6rFAAAAAAAABo8v/ymEQAAAAAAAOoXi0YAAAAAAAAQLBoBAAAAAABAsGgEAAAAAAAAwaIRAAAAAAAABItGAAAAAAAAECwaAQAAAAAAQDTxumFQUFBdjgO4oMrKyvoewgUxL1Bf/HVeMCdQX5gTgC9/nRPOMS9Qf/x1XjAnUF+8zAm+aQQAAAAAAADBohEAAAAAAAAEi0YAAAAAAAAQnjuNAADAjwsNDZWsa9eukg0dOtTncUlJiWyzefNmyY4ePSpZWVlZNUYIAABgdymFhIRIVlpaejmGAz/FN40AAAAAAAAgWDQCAAAAAACAYNEIAAAAAAAAgkUjAAAAAAAACIqwgQbCKrJr166dZNddd51kVgHv7t27JcvOzq7h6IArX5MmesqMj4+XbMKECZLddtttknXq1MnncUVFhWxjzbm8vDzJysvLJfPq3LlzkhUWFkq2bt06yVasWCFZfn5+jccCAADqRseOHSW74447JBs/frxkJ0+elMy6HrGuAaztsrKyJNu/f79k6enpkhUUFEiGusU3jQAAAAAAACBYNAIAAAAAAIBg0QgAAAAAAACCRSMAAAAAAAAIirCBBiI0NFSymTNnSjZo0CDJrBLdd999V7KPPvpIstzcXK9DBOpd48aNJYuOjpYsJiZGsgEDBkg2YsQIyXr16iWZNU/mz5/v8zgkJES26d27t2QtWrSQzCrC98p6Tay/4dprr5XMep0WLVokWUZGhmRWATcAAKgb1vneummOVYSdnJwsmddCautGItbNP7wWZh8+fNjnsXWNYWUHDx6U7Pjx45KVlpZKFuj4phEAAAAAAAAEi0YAAAAAAAAQLBoBAAAAAABAsGgEAAAAAAAAQRF2PbFKS6+55hrJiouLJata/uUchV2BplEjXe+NioqSLDExUbKjR496+tlp06ZJVlhYKNny5cslY3/E5RYWFiZZnz59JLOK4GNjYyXr0qWLZN26dZPMKpFfv369ZElJSZJ98cUXPo+tIuyrr75astatW0tmlVt6ZRVUduzYUbIpU6ZIdv/990tmnd8WLlwo2YEDB7wOEQBwGVnXmdY5yjreW59d4B9Onz4tmfW58uzZs5JZ1zb79u2TzLoeCQ8Pl8z67NG+fXvJ4uPjJevfv7/P47KyMtnmxIkTkll/a2ZmpmTp6emSWX+rtV1D3f/5phEAAAAAAAAEi0YAAAAAAAAQLBoBAAAAAABAsGgEAAAAAAAAQRF2PWnatKlkU6dOlezMmTOSLV26VLLU1FTJzp07V7PBwe+FhoZKFhcXJ1lkZKRkr732mmQRERGSjRo1SjKr9DY7O1uyjRs3Ssb+iNpiFSoOHjxYsgceeECyG264QTKrGDI/P1+ylJQUyVavXu0pO3bsmGRVFRUVSfbVV1/96M/VBasce+/evZK98MILkt11112SpaWlSUYR9pXPKjy1rm+Cg4Mls4p2vTy/9VzWOfFSMqtY3/q7rGPHt99+62k71C7rOsYq1bXe74bA2j+tzPr7rYJraw60aNFCssrKSskOHjwomXXTCOtnLdb8sc6pWVlZnp4vkFkF11Zxc0VFhWTW9cjKlSslO3XqlKextGzZUrLOnTtL1qtXrx/NrG06deokmXUNeOutt0qWl5cnWXJysmRbtmyRzCrMPnTokGRWUbc1T/wF3zQCAAAAAACAYNEIAAAAAAAAgkUjAAAAAAAACBaNAAAAAAAAICjCridWQfGwYcMk69Kli2QlJSWSWQVbBQUFNRwd/F3z5s0lGzhwoKef3b59u2Q7duyQzNrP7rnnHsl+8YtfSHb48GHJMjIyPI0P+CGrjPP666+XbObMmZJde+21ki1btkyyjz76SDKr8NAq2bTmSUNglWCuWrVKsj59+kj22GOPSda9e3fJrELW0tJSr0PED1iF0V4Lb4OCgiSzitCt806bNm0ka9u2rWRW4an1O6pq1qyZp+eyrqmscVjjtQqUW7du7el3WIWviYmJku3cuVMyq5AW3lgl6gMGDJBs2rRpklnvd0O4UYdVUm3NFetGEtZ21ny3irUtVhGwVbZsFWFb761VGJyUlCTZP/7xD8nKysouNMyAZB13reOs9d5Y7+GlzJ3CwkLJrBuOWJkXVhF+7969JbM+P1mfyceOHSvZHXfcIZl10xDrBikbNmyQ7MiRI5Ll5uZKZr0XdX1O4ZtGAAAAAAAAECwaAQAAAAAAQLBoBAAAAAAAAMGiEQAAAAAAAARF2HWgaombVUZ53XXXSWYVkcXGxko2cuRIyawyrc2bN190nLhyWQWFQ4YMkezkyZOSWYXUVpH6vHnzJLPKQSdPniyZVUr81FNP+Tym8BZVWYW8/fv3l+zhhx+WLCEhQbIXX3xRsldffVWyM2fOeBwhqrLOM9OnT5esZ8+eknXt2lWy3bt318q4rkTW/h8cHCyZVWZtlTRbJeXWNYVVPBsdHS2ZVSxv/Q5rLFapqhdeS1at5y8qKpLMOidaBfdpaWmSWedJqwTVur6zbg6Rk5MjGbyxypxHjBgh2QMPPCCZVTR7+vTpWhlXfbL+BqtAev/+/Z62s57Pet3btWvnaTvr+Gaxrm8HDx4sWVRUlGRr166VzPp7A5n1WTMmJkYy69hrHQP9uWg8OzvbU/b5559LZr1O8fHxkk2ZMkWyiRMnSvbCCy9IZt1IwdqHlyxZItmmTZskO3bsmGTl5eWS1RTfNAIAAAAAAIBg0QgAAAAAAACCRSMAAAAAAAAIFo0AAAAAAAAgKMK+RE2a6EvYqVMnn8f/9V//JdtMmjRJMqt0zipP7NWrl2QTJkyQbMuWLZLVtIwS9ccqD2zTpo1kN998s2SffvqpZFbxmsUqD3znnXc8jeWee+750ed74403ZBsKiQObVZ45evRoyazS9/fff18ySq/r3rfffivZ0aNHJbMKmOPi4iQL5CJsq0B6/PjxklnHV6uk2iqUta5ZLBUVFZLt3btXshUrVkiWmpoqmVW0W1JSUqNx5ObmSmaVmxYUFEhmlYJ6Ldu2Cn+tgl7rmPXNN99IRhF2zXXu3NlTtm3bNskef/xxyRpCWbJ1fW/t29Z2XjPretRr5pVVQGy9Z1OnTpVs6NChkjWE97Y2WddZ1udK6/hplcjXZtGyPykuLpbMut5JSUmR7KWXXpLsxhtvlMy6kdAtt9wi2Z133inZxo0bJZszZ45kS5culaym+KYRAAAAAAAABItGAAAAAAAAECwaAQAAAAAAQLBoBAAAAAAAAEERdjVYJYg/+clPJKta2Na1a1fZJikpSbJ3331XstDQUMnuv/9+yUaNGiXZhg0bJLNKK+Hf2rZtK9n1118vWatWrSSz3m+r3M5iFSju2LFDsrlz50rWvn17yZ5++mmfx4cOHZJtrOLuwsLCi44TDYdVIHvbbbdJZhVb/u1vf5OM0uu6Z5UZ79mzR7KbbrpJsu7du9fJmK4EvXv3luypp56SbPjw4ZLl5+dLtmjRIsl27twpmTV3rCLT0tJSyaybKFjHZ2ufsAqtvRZQe3ku6284e/ZsjZ6/OqxruRtuuEEyq6TZKlC9HGNuCKzX07rusArSraJ2a39H/bCOH9axp1Ej/d5D06ZN62RMDUl0dLRk/fr1k8wqWrbem0C6wZJ1fLZeE+t4smbNGsmsGyRUvZmWc87dfvvtklnrD4899phk1jFw8+bNknnBN40AAAAAAAAgWDQCAAAAAACAYNEIAAAAAAAAgkUjAAAAAAAACIqwnXMhISGSDRs2TLL77rtPsoEDB0pWtXz1t7/9rWzz1VdfSbZ3717JGjduLJlVyP3II49INmPGDMm2bt0q2cmTJyULpGIzf9ehQwfJbr75Zslyc3Mls4rsTp8+XeOxWIVvW7ZskWzOnDmS/fGPf/R5/Mwzz8g2OTk5np6/uLj4ouOE/7MKKwcNGiRZy5YtJVuwYIFk+/btq52BoVqsc4V13rK2C+TS3x49ekiWkJAg2XfffSfZe++9J5lVPm6dE6xCWauQ2npvalpc3VBYxfqffPKJZLfccotkcXFxkn399deSHTt2rIajCyxWEXazZs0kS05OlozSa//WokULT1lZWZlk1k0CAllERIRk1rnH2m7dunWSWTccgLKud4qKijxlVnG19XyxsbGSWeeZsLCwC46zuvimEQAAAAAAAASLRgAAAAAAABAsGgEAAAAAAECwaAQAAAAAAAARcEXYVqnq2LFjJZs+fbpk3bp1k8wqll6xYoXPY6v0Oi8vTzKvJZNWOZlVbDZp0iTJ7rrrLsnefvttyazCR9S9Ro10HbdTp06SWYWp1n5mFapVVFTUbHAXYBUPWvvoyy+/7PN41qxZss3PfvYzyazy7W+++UYyCvquLFaxZXR0tGRWcbtVbmqVYqLuWYXmHTt2lMyax9bxKVBYRdPW8f/IkSOSffvtt5IdOnSodgaGC7LOnZs3b5assLBQsl69eknWvn17ySjCVsHBwZJZRdhNmuhHmv3799fJmFB32rRpI1lkZKRk1rWBdbwMZNbnh759+0pmnZ8///xzybjOrnvWa2zdXMQ6Llqfx9LT02tlXM7xTSMAAAAAAAAYWDQCAAAAAACAYNEIAAAAAAAAgkUjAAAAAAAAiAZdhG0Vp40bN04yq/TaKmL7+OOPJVu4cKFkO3fu9DrEGjlw4IBkn376qWRDhgyRLDExUbKVK1dKZpVqWsWdqF3WPhsfHy9Z69atJVu2bJlk9VUOnJubK9mCBQt8HlvF8lOnTpXsvvvuk8wq7bPKkSsrKy86TtQfqwg7NDRUMqvs0rqRAOqHVXrdoUMHyayC38zMzDoZ05UgLS1NMus1so6TVqkyRdh1zzqfWGXuhw8flswqbrbmyY4dO2o4uoarXbt2klnHHetcsXfv3joZE+qO9X5HRUVJZr3fBw8erJMxXams407v3r0ls8qSrRJ5rzdsQu2yzvnWjWOs4511PqopvmkEAAAAAAAAwaIRAAAAAAAABItGAAAAAAAAECwaAQAAAAAAQFyRRdhBQUGShYeHSzZhwgTJHn74YcmaNNGXoWppr3POvf/++5JlZWVdcJx1xSo3Tk1NlWz58uWSPfvss5KNGDFCsqVLl0qWn5/vaXyoue7du0s2cOBAyawi4HXr1klWUVFROwOrJqssr2o59ssvvyzbxMTESDZmzBjJrH2xoKBAsoyMjIuMEvWpUSP9/yysjDJz/9anTx/JIiIiJNu2bZtkgVzebBWPWq+RdfwbO3asZCdOnJDMOg6npKRIxhyrOesGIdu3b5fMes9iY2Mla9asmWTFxcU1HF3DYL1OVlmyVSRv3TgG/s16b61zinUdbB0HA0Xz5s0li4uLk8wqFV+yZIlkZ86cqZ2BoVqaNm0qmXUMtLazbsRVm58D+aYRAAAAAAAABItGAAAAAAAAECwaAQAAAAAAQLBoBAAAAAAAAOH3RdhW6XVYWJhkI0eOlOw3v/mNZIWFhZLNmTNHMqsUzCra9RdWIXdSUpJkVhH4gw8+KNnmzZslO3XqlGRW0Sa8sUp/e/ToIVnv3r0ls8pM/b3wsWrZ6uHDh2Wb//mf/5Hs97//vWR33nmnZFWLtp1z7u9//7tk1n6My6+oqEiy06dPS9a+fXvJrCJH1A+rCDskJEQyq/T6+PHjdTKmK0F5eblkH3/8sWTW6zt69GjJrrrqKsmsAuX/+I//kMwqj6Ucu+Y+//xzyUaNGiVZQkKCZNbNMKxy00DSs2dPyayb33z//feSHT16tE7GhLpjnd9DQ0MlS0tLk8w6rgYKa55Yx5iSkhLJrBvpoH5Y17zWecG6Xk5OTq6TMf0L3zQCAAAAAACAYNEIAAAAAAAAgkUjAAAAAAAACBaNAAAAAAAAIPy+CNsq1IyPj5fMKtC1yr5+97vfSWYVRpeWlnodol84e/asZFbR8HvvvSeZVRhuladZZdv+XA7u71q2bClZXFycZFbh45o1a+pkTPVty5Ytkv3v//6vZDNnzpTs3nvvlcwqwfzggw8ks+YP6tbJkycls8qS+/XrJ5lVDr9s2bLaGRiqxTofW2Xz2dnZklVUVNTJmK5U33zzjWRffPGFZNb5+Z577pHMOv5t3bpVsrfffluysrKyCw0TP+LLL7+UbNeuXZJdc801kg0bNkwy68YXgVRUbt0gxDpnW+ePQC5GvlJZRdiNGzeWLJBLzq2bRA0ePFgy6/xsHU82btxYOwPDJbOueWNiYiSziuDr+qYJfNMIAAAAAAAAgkUjAAAAAAAACBaNAAAAAAAAIPyq06hRI13Dsv4t81/+8hfJIiIiJHv66aclW79+vWRXWn+RVzk5OZLNmzdPskceeUSyadOmSbZ3717Jtm/fXsPRoU+fPpJZ/5b1+PHjkq1du7ZOxuSPPv74Y8msf/P++OOPS/bkk09KZvUe8O+5/UNGRoZkZ86ckczq/kL9sLrZrHM5fpzVU7No0SLJIiMjJfv1r38tWdu2bSWbMWOGZO+++65kdBrVnPXarV69WjKrm+3aa6+VzLq+zcvLq9ng/Jx17IiNjZXMum4/ePBgnYwJl5d13AoODpYskDuN2rRpI9mAAQMks84p69atk4x+Qf9hdd2FhYVJduDAAcmsrtDaxJUdAAAAAAAABItGAAAAAAAAECwaAQAAAAAAQLBoBAAAAAAAAOFXRdjx8fGSWWXWVgnqrFmzJLNKr0+dOlXD0V15rGKz7Oxsyd5//33JJk+eLFn//v0l279/v2SB9BpfCqv0ulOnTpIlJydLlp6eXidj8kdWqeiHH34oWXh4uGRWyfuzzz4r2aOPPirZ4cOHJbNKBVF7rCJsq9ivS5cuklnlmXVdCgj7PGsVOU6cOFGyzMxMySil92XdCGHhwoWSnTt3TjLrhhbdu3eXrHPnzpJZN74oLy+/4Dhxcbt27ZLs2LFjklkFt9HR0ZI11CJs6zzeqlUryQoKCiQ7ceJEnYwJdcfa362i/5KSEskCuQj7xhtvlOzqq6+WzDqOf/HFF3UyJlRfu3btJBs8eLBk1ufqlJQUyer6MwrfNAIAAAAAAIBg0QgAAAAAAACCRSMAAAAAAAAIFo0AAAAAAAAg6q0I2ypjnDJlimTDhg2T7J133pFs8eLFklmleFZZZCCxyuSs13PMmDGSjR49WrLU1FTJtmzZUsPRNVxWkWOfPn0ka9RI13G3bt0qmVUOHUisuW0dA6zXPTExUbLf/va3klkl/Dk5OZIF+jGlNlkF71Yh+ZAhQySzbqRglTSjdi1ZskQy69g2YMAAyYYOHSrZpk2bJAvkAvqzZ89KZt2AwrqhhXW+f+655yTr27evZIcOHZKMIuyas45jVsl57969JbPm0+7du2tnYH7GKkYODQ2VzCoR5yYsV5727dtL1rp1a8ms4vOsrKw6GdOV4Prrr5esSRP9SL9jxw7JArlA3N/cdtttknXr1k2yzz77TLLt27fXyZguhm8aAQAAAAAAQLBoBAAAAAAAAMGiEQAAAAAAAASLRgAAAAAAABCXpQjbKueySjFHjRol2YEDByR78803JTty5IhkFNQqq1QzJSVFsrVr10p2ww03SHbrrbdKdvDgQckCubDOObukNy4uTrLs7GzJKBZXVjGuVaL84YcfSmYVL06cOFEyq2z2rbfekswq5ETNWPt/ZmamZCNGjJCsf//+klGEXfes4/2nn34q2dVXXy3ZNddcI1nHjh0ls87vgezMmTOSpaWlSbZhwwbJKioqJLPORevWrZOMouGay8/Pl2zv3r2SWdcKCQkJklnntobAKkEOCQmRzNoXi4qK6mRMqDvWeSEqKkqyXbt2SRYo5wXrhi7WMds6xliF+dzQoH40a9ZMsjvuuEOy06dPS7Zt2zbJ6qPQnG8aAQAAAAAAQLBoBAAAAAAAAMGiEQAAAAAAAASLRgAAAAAAABCXpQi7a9eukg0fPlyyiIgIyd555x3JrOJm1JxVqrlgwQLJevToIdnIkSMlO3TokGSLFy+WrLi42OsQr3jXXnutZG3atJHsyy+/lGzfvn11MqaGxir3s8oT//GPf0jWvXt3yR5++GHJrOLF5cuXS3by5MkLDRMXYR0TDh8+LFlJSYlkVhE26seePXsky8vLk8wqPLWyQCk8vRRWwXVBQYFkVlmwdXOA4ODg2hkYnHP2+2MVYRcWFkrWq1cvycLCwiSzjotXmrZt20oWGhoqmfU6UdTu36ybIg0aNEiypk2bSmZdB1vFzw2R9dmrQ4cOklnXu9YNYlD3rH198ODBklk3ObBuJJKcnCxZfRSa800jAAAAAAAACBaNAAAAAAAAIFg0AgAAAAAAgGDRCAAAAAAAAKLWi7Ct8ier9NoqBrZKvD755JPaGRiqZePGjZJ9+OGHklllwXfddZdkGRkZkm3YsEGyyspKjyP0X0FBQZL17dtXsrNnz0pmFWOePn26dgYWgKxi5e3bt0v20ksvSfbyyy9LNmPGDMmsMsa1a9dKRklnzWRlZUlmFSP36dNHstatW0tmvV8N4bjjT0pLSyWzioAbN258OYYTsKzX3DoOtWzZUjLem7pn3TTkxIkTksXExEhmFeEeOHCgdgZWj6x90fpcYZXA1kcxLLyz9uPrrrtOsuzsbMms6zbrGrohioyMlMwqC7cyq0S+USP9vsi5c+dqODpYrBsV3HfffZ5+1vps7C+F5nzTCAAAAAAAAIJFIwAAAAAAAAgWjQAAAAAAACBYNAIAAAAAAICo9SJsq5zvpptukswqiUpKSpLMKjxF3bNK0RYuXChZp06dJLv77rslS0xMlGznzp2SWSW1V5pmzZpJ1q1bN8kKCwslswrDUbuKiooks449r7/+umT//u//Ltn9998vWUFBgWRWuV1ZWdkFx4n/LzMzUzKrMD4hIUGyW2+9VbJly5ZJxvtQu9q2bStZ8+bNJbOOgVaGmrGKsK1jU3h4uGRWqWpwcLBk1o0frKLVkJAQyax9ourv9VrIbY3DYo3tUrbz+rNWZs0Tq5S/VatWknXv3l2yhlCEbZ2freOzde3Zs2dPyfbv3y+ZdYwJlFLlumDNPetYMXbsWMmsz4xLly6VLDk5uWaDawCsEuR9+/ZJ1q9fP8msz2PWeeHYsWOS5eXlSXbmzJkLjjNQWft6586dJRs/frxk1k1zUlJSJLNu6lMf+KYRAAAAAAAABItGAAAAAAAAECwaAQAAAAAAQLBoBAAAAAAAAHFJRdhW+dntt98umVVQun37dsmsgtKGymtRovUaey18vJTtrKy8vFyylStXSmaVEQ4cOFCywYMHS2YVEl9pYmNjJYuKipJs165dkh06dKhOxoSLs8r9XnvtNcmsQvP77rtPMqts9vjx45KlpqZKZhWhBjKrBNIqD5w4caJk//mf/ynZpk2bJMvOzpbMuhkAvLEKOdu1ayfZnj17JDt8+HCdjCkQWfvwqVOnJGvdurVkXbp0kSw0NFSyJk30MtIquLbOiwMGDJCsasGz9Tut6xNrHFb5trWdde1lFYFbz2exftbKrJJv63dYxf8tW7b0NJYrzVdffSWZdZ6cMGGCZF27dpVswYIFkq1bt04y6/xslWNb52drnvnzedzr5w8r87rPWkXlkydPlqy0tFSy77//XrITJ05IFiisMnfrRi0zZ86U7Gc/+5lkd9xxh2TLly+XzLr5kXUsst5D6/OiP8+JS2GdP6dOnSpZRESEZB999JFkBw8erJVx1QW+aQQAAAAAAADBohEAAAAAAAAEi0YAAAAAAAAQLBoBAAAAAABAXFIRdmRkpGRWEXZFRYVkX375pWRWEV1DEBYWJplVEhcTEyOZVR5qlUxaJYstWrSQzCqxs7az3ltrfFVLKy+UpaSkSGYVcjYEVhmj9Rpb5btWhvphlfb9/ve/l8zrcdAqx/7d734nWV5entchBgSrjHTHjh2SvfXWW5L94Q9/kGz27NmS/fd//7dklNJ7YxUVDx8+3NPPWjcDsErpUTPW3CkqKpJsyJAhki1ZsqTGv9frTTis97pqCah13LT+Lus6s7CwULKysjJPY7PKja3Xzmu56+nTpyWzxmxlVhG0dTOAhsB6z6zzpFUOfPfdd0v2xhtvePod1mtsFTJb54WjR49KZr3f9XFzBeua37oxi3WTD6u83rq+91qab30Wst4f6/MCfG3ZskUyqwh71KhRkk2ZMkUyqzD7sccek2zz5s2SWYXZq1atksyaJw2hHNsquL7zzjsl27Ztm2TWtaw/fzbmm0YAAAAAAAAQLBoBAAAAAABAsGgEAAAAAAAAwaIRAAAAAAAAxCUVYQ8aNEgyqxCtb9++ks2aNUuy6dOnS2aVIPqT4OBgyaoW41pFuSEhIZ6ey8oaNdK1Pqt40irAs0rHrDJKq8QvNzdXsoyMDMk+//xzydavXy9ZcnKyZA2BVdLZEMreYM+BV155RTKrrH7MmDGSZWZmSvbqq6/WbHABxCqMt4p7rSLPqVOnSmbN2ddee02ynTt3SmYV1waSSZMmSXbddddJZhVoWucF1J7S0lLJrJLZfv36SWbdrCQ9PV2ykpISycrLyyU7duyYZPv27ZOsaknxpcwva157ZZ2zrSJjr+d2rz9rZdb76M9lqbXN2u/++te/SrZy5UrJrH174MCBksXFxUl26623ShYeHn7Bcf7Qpex7dc36bGDtY9bNiaoW1Tvn3NatWz1tZ31e+PrrryWzrovgy9q/rNf3//7v/ySzSvStgnNr/x89erRkzz77rGQ//elPJbOuAT755BPJNm3aJFl+fr5k9cH6TG7dsKpHjx6SzZkzRzJ/X+Ooim8aAQAAAAAAQLBoBAAAAAAAAMGiEQAAAAAAAASLRgAAAAAAABBBlR5b/Kyi5auuukqy2267TbL+/ftLFh0dLVmrVq0kswrb/ImXYmmr6OrkyZOSWaVzVtlhcXGxZGVlZZ4yq1TSKlSzftYqvLQKs60xW3+vVSps8ecSaWte3HDDDZJZBWgRERGSpaamSma93/XF2lesAsWioqIfzbxs45wWo1bnZ2s7s8ZivSbjx4+X7NFHH5XM2rd/85vfSLZt2zZPP+sPrDlxOVg3F7DKTX/5y19KdvPNN0u2Z88eyebNmyfZ6tWrJcvLy7vgOGuDVbI4dOhQydq2bSuZdZ6xCjRzcnIk69Onj2S//vWvJWvatKlkr7/+umSLFi2SzDrPeMWc8GW9D127dpWsU6dOkln7iXX8s64prNJnr2XO1s+i5vx1TjhX+/PCKqm1PldERUVJ1qZNG8latmwpWYsWLTz9Dut8VB/HAev9t65trKJha75b1/y1fZ11OW4u4a/z4nLsI9bvCA0NlaxDhw6SWTe7sm6KZV2PWGXb1k0Tdu3aJdnGjRsl27Jli8/jtLQ02cbahy/lHGNdU02ePFky60Yq9957r2Rr1qyRzBrz5eBlTvBNIwAAAAAAAAgWjQAAAAAAACBYNAIAAAAAAIBg0QgAAAAAAADikoqwrSwyMlIyqzjKKgEODw+XzCq28ydeioGtUisru5TSa2scXgsq/Z2/FtY5Z88Bq+T93/7t3yS78cYbJbPmgD/9/dbf26iRrj1bWdVSe6vk3utzWa+JVahXtZTeObuQ1et2XjPrfRw4cKBklueff16y+fPnS+ZP+8UP1Vfpr8Xax6655hrJpkyZItntt98umVXauWrVKsmSkpIk27t374+Or3PnzrLNTTfdJJnXkslmzZpJZu2vXsvmrcJkq0Dzn//8p2QLFy6UzCrgvhTMCcCXv84J567MeWFdj1jHwCZNmkjmL0XY1vWO9bnCn/edS+Wvf9uVOCeswuxevXpJ1rdvX8msG2X17t1bsrCwMMmysrJ8Hh87dky2OXr0qGRHjhyR7MSJE5JZ10XWGse4ceMks64fhwwZIpl1DXQ5iuAtFGEDAAAAAACgRlg0AgAAAAAAgGDRCAAAAAAAAIJFIwAAAAAAAIhLKsIGLgd/Laxzzvu86Nixo2RWca1VsuZPc88qFm7atKlkzZs3l6xqKa+XbS6UWaV4VhYSEiKZVVp5KZn1O6z3zNru0KFDkv3lL3+RbPny5ZL567zwp/3Vq5iYGMkmTpzoKbPm7J49eySraRG2VR5ZUlIi2e7duyU7fvy4ZC1btpQsKipKMms+WSXaGzZskGzp0qWSZWZmSlbb+zBzAvDlr3PCOeYF6o+/zouGPCesawrrc9CAAQMkS0hIkKzq9VK7du1kG6uQ3iqa9nozHOvziFUEfvDgQcmmTp0qmVW2XV8owgYAAAAAAECNsGgEAAAAAAAAwaIRAAAAAAAABItGAAAAAAAAEBRhw+/5a2Gdc8yL+mIVclsl1S1atPCUWeXAl7KdlVnF3zk5OZKtX79esiNHjkjmr/OiocwJq7Rx5MiRklnl2P3795csIiJCsvLycp/HxcXFsk1KSopkq1atkuzLL7+UzNpvrHkSHR0tWXh4uGRWaaP1O6xSycuBOQH48tc54RzzAvXHX+cFc8JmXY916dLF53GPHj1km27dukkWGxsrmVWibV3HW8Xa+fn5kn388ceSffjhh5JVvQasTxRhAwAAAAAAoEZYNAIAAAAAAIBg0QgAAAAAAACCRSMAAAAAAAAIirDh9/y1sM455gXqj7/Oi0CbE61atZKsZ8+ekl199dWSnT592ufx0aNHZZvvv/9eMqswG8wJoCp/nRPOMS9Qf/x1XjAn/Id1w52QkBDJrPes6rXdlYAibAAAAAAAANQIi0YAAAAAAAAQLBoBAAAAAABAsGgEAAAAAAAAQRE2/J6/FtY5x7xA/fHXecGcQH1hTgC+/HVOOMe8QP3x13nBnEB9oQgbAAAAAAAANcKiEQAAAAAAAASLRgAAAAAAABAsGgEAAAAAAEB4LsIGAAAAAABA4OCbRgAAAAAAABAsGgEAAAAAAECwaAQAAAAAAADBohEAAAAAAAAEi0YAAAAAAAAQLBoBAAAAAABAsGgEAAAAAAAAwaIRAAAAAAAABItGAAAAAAAAEP8Pl0YBsgjrftQAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1200x200 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "preview_loader = get_dataloader(batch_size=6, img_size=32, max_items_per_class=64, augment=False, num_workers=0)\n",
        "imgs, prompts = next(iter(preview_loader))\n",
        "fig, axes = plt.subplots(1, 6, figsize=(12, 2))\n",
        "for i in range(6):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(((imgs[i].squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
        "    ax.set_title(prompts[i][:10] + \"...\")\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "072a879a",
      "metadata": {},
      "source": [
        "## Diffusion helpers\n",
        "Cosine noise schedule with sinusoidal time embeddings. The helper functions implement the forward process and sampling utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4495394",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "T = 300\n",
        "betas = cosine_beta_schedule(T)\n",
        "alphas = 1.0 - betas\n",
        "alpha_cum = torch.cumprod(alphas, dim=0)\n",
        "alpha_cum_prev = torch.cat([torch.ones(1), alpha_cum[:-1]])\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t.cpu()).float().to(t.device)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "    return extract(alpha_cum.sqrt(), t, x_start.shape) * x_start + extract((1 - alpha_cum).sqrt(), t, x_start.shape) * noise\n",
        "\n",
        "def p_mean_variance(model, x, t, t_emb, text_emb):\n",
        "    pred_noise = model(x, t_emb, text_emb)\n",
        "    beta_t = extract(betas, t, x.shape)\n",
        "    alpha_t = extract(alphas, t, x.shape)\n",
        "    alpha_cum_t = extract(alpha_cum, t, x.shape)\n",
        "    coef1 = 1 / torch.sqrt(alpha_t)\n",
        "    coef2 = beta_t / torch.sqrt(1 - alpha_cum_t)\n",
        "    mean = coef1 * (x - coef2 * pred_noise)\n",
        "    var = beta_t\n",
        "    return mean, var, pred_noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_emb, text_emb):\n",
        "    mean, var, pred_noise = p_mean_variance(model, x, t, t_emb, text_emb)\n",
        "    if (t == 0).all():\n",
        "        return mean, pred_noise\n",
        "    noise = torch.randn_like(x)\n",
        "    return mean + torch.sqrt(var) * noise, pred_noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ca70e9",
      "metadata": {},
      "source": [
        "## Text encoder and tiny U-Net\n",
        "GRU-based text encoder feeds a light U-Net. Time and text embeddings are injected into each block to keep conditioning stable. An EMA copy of the network is maintained for sharper sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a19c9a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim=128, hidden_dim=192):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(1024, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        emb = self.embedding(tokens)\n",
        "        _, h = self.rnn(emb)\n",
        "        return self.proj(h.squeeze(0))\n",
        "\n",
        "def tokenize(prompts: List[str], vocab=None, max_len=16):\n",
        "    if vocab is None:\n",
        "        vocab = {}\n",
        "    token_lists = []\n",
        "    for text in prompts:\n",
        "        tokens = []\n",
        "        for word in text.lower().split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab) + 1\n",
        "            tokens.append(vocab[word])\n",
        "        token_lists.append(tokens[:max_len])\n",
        "    max_len = max(len(t) for t in token_lists)\n",
        "    padded = []\n",
        "    for t in token_lists:\n",
        "        padded.append(t + [0] * (max_len - len(t)))\n",
        "    return torch.tensor(padded, dtype=torch.long), vocab\n",
        "\n",
        "def sinusoidal_embedding(n, d):\n",
        "    pos = torch.arange(n)[:, None]\n",
        "    dim = torch.arange(d)[None, :]\n",
        "    angle = pos / (10000 ** (2 * (dim // 2) / d))\n",
        "    emb = torch.zeros((n, d))\n",
        "    emb[:, 0::2] = torch.sin(angle[:, 0::2])\n",
        "    emb[:, 1::2] = torch.cos(angle[:, 1::2])\n",
        "    return emb\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(1, out_ch),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class TinyUNet(nn.Module):\n",
        "    def __init__(self, base=32, time_dim=128, text_dim=192):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_dim, time_dim), nn.SiLU())\n",
        "        self.text_mlp = nn.Sequential(nn.Linear(text_dim, text_dim), nn.SiLU())\n",
        "        cond_channels = {\"d1\": base, \"d2\": base * 2, \"mid\": base * 4, \"u1\": base * 2, \"u2\": base}\n",
        "        self.time_proj = nn.ModuleDict({k: nn.Linear(time_dim, v) for k, v in cond_channels.items()})\n",
        "        self.text_proj = nn.ModuleDict({k: nn.Linear(text_dim, v) for k, v in cond_channels.items()})\n",
        "\n",
        "        self.down1 = ConvBlock(1, base)\n",
        "        self.down2 = ConvBlock(base, base * 2)\n",
        "        self.to_vec = ConvBlock(base * 2, base * 4)\n",
        "        self.up1 = ConvBlock(base * 4, base * 2)\n",
        "        self.up2 = ConvBlock(base * 2, base)\n",
        "        self.skip_proj = nn.Conv2d(base, base * 2, 1)\n",
        "        self.out = nn.Conv2d(base, 1, 1)\n",
        "\n",
        "    def _inject(self, feat, name, t_feat, txt_feat):\n",
        "        t = self.time_proj[name](t_feat)[:, :, None, None]\n",
        "        txt = self.text_proj[name](txt_feat)[:, :, None, None]\n",
        "        return feat + t + txt\n",
        "\n",
        "    def forward(self, x, t_embed, text_embed):\n",
        "        t_feat = self.time_mlp(t_embed)\n",
        "        txt_feat = self.text_mlp(text_embed)\n",
        "\n",
        "        d1 = self.down1(x)\n",
        "        d1 = self._inject(d1, \"d1\", t_feat, txt_feat)\n",
        "        d2 = self.down2(nn.functional.avg_pool2d(d1, 2))\n",
        "        d2 = self._inject(d2, \"d2\", t_feat, txt_feat)\n",
        "        mid = self.to_vec(d2)\n",
        "        mid = self._inject(mid, \"mid\", t_feat, txt_feat)\n",
        "        u1 = nn.functional.interpolate(self.up1(mid), scale_factor=2, mode=\"nearest\")\n",
        "        skip = self.skip_proj(d1)\n",
        "        u1 = self._inject(u1 + skip, \"u1\", t_feat, txt_feat)\n",
        "        u2 = self.up2(u1)\n",
        "        u2 = self._inject(u2, \"u2\", t_feat, txt_feat)\n",
        "        return self.out(u2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1743c20",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 255\n"
          ]
        }
      ],
      "source": [
        "arr = np.load(\"data/circle.npy\", mmap_mode=\"r\")\n",
        "print(arr.min(), arr.max())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8139cb76",
      "metadata": {},
      "source": [
        "## Training loop\n",
        "The model predicts the noise added at timestep *t*. EMA smoothing, gradient clipping, and a cosine learning rate schedule stabilize training. Default settings keep each epoch to the ~6k sketches cap; increase `max_items_per_class` to use the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96394d7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_encoder = TextEncoder().to(device)\n",
        "model = TinyUNet().to(device)\n",
        "ema_model = TinyUNet().to(device)\n",
        "ema_model.load_state_dict(model.state_dict())\n",
        "ema_decay = 0.995\n",
        "\n",
        "optimizer = torch.optim.Adam(list(model.parameters()) + list(text_encoder.parameters()), lr=2e-4)\n",
        "vocab: Dict[str, int] = {}\n",
        "\n",
        "def get_time_embedding(timesteps, dim=128):\n",
        "    emb = sinusoidal_embedding(max(T, timesteps.max().item() + 1), dim).to(device)\n",
        "    return emb[timesteps]\n",
        "\n",
        "def update_ema(model_src, model_tgt, decay):\n",
        "    with torch.no_grad():\n",
        "        for p_src, p_tgt in zip(model_src.parameters(), model_tgt.parameters()):\n",
        "            p_tgt.mul_(decay).add_(p_src, alpha=1 - decay)\n",
        "\n",
        "def p_losses(x0, prompts):\n",
        "    b = x0.shape[0]\n",
        "    t = torch.randint(0, T, (b,), device=device).long()\n",
        "    noise = torch.randn_like(x0)\n",
        "    x_noisy = q_sample(x0, t, noise)\n",
        "    global vocab\n",
        "    token_batch, vocab = tokenize(list(prompts), vocab)\n",
        "    token_batch = token_batch.to(device)\n",
        "    text_emb = text_encoder(token_batch)\n",
        "    t_emb = get_time_embedding(t)\n",
        "    pred = model(x_noisy, t_emb, text_emb)\n",
        "    return nn.functional.mse_loss(pred, noise)\n",
        "\n",
        "def train(epochs=5, grad_clip=1.0):\n",
        "    global ema_model\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(dataloader), eta_min=5e-5)\n",
        "    for epoch in range(epochs):\n",
        "        loop = tqdm(dataloader, desc=f\"epoch {epoch+1}\")\n",
        "        for imgs, prompts in loop:\n",
        "            imgs = imgs.to(device)\n",
        "            loss = p_losses(imgs, prompts)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(list(model.parameters()) + list(text_encoder.parameters()), grad_clip)\n",
        "            optimizer.step()\n",
        "            update_ema(model, ema_model, ema_decay)\n",
        "            lr_scheduler.step()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "        torch.save({\"model\": model.state_dict(), \"ema\": ema_model.state_dict(), \"text\": text_encoder.state_dict(), \"vocab\": vocab}, \"shape_diffusion.pt\")\n",
        "        print(f\"Epoch {epoch+1} checkpoint saved\")\n",
        "\n",
        "# Kick off training (adjust epochs upward for higher fidelity once you are satisfied with runtime)\n",
        "train(epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419ad08d",
      "metadata": {},
      "source": [
        "## Sampling from the model\n",
        "Start from random noise and apply the reverse diffusion steps conditioned on a prompt. EMA weights are preferred; if training is skipped, fall back to the current model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab72936",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(prompts: List[str], use_ema: bool = True):\n",
        "    model_to_use = ema_model if use_ema else model\n",
        "    model_to_use.eval()\n",
        "    global vocab\n",
        "    token_batch, vocab = tokenize(prompts, vocab)\n",
        "    token_batch = token_batch.to(device)\n",
        "    text_emb = text_encoder(token_batch)\n",
        "    b = len(prompts)\n",
        "    img = torch.randn(b, 1, 32, 32, device=device)\n",
        "    for i in tqdm(reversed(range(T)), desc=\"sampling\"):\n",
        "        t = torch.full((b,), i, device=device, dtype=torch.long)\n",
        "        t_emb = get_time_embedding(t)\n",
        "        img, _ = p_sample(model_to_use, img, t, t_emb, text_emb)\n",
        "    img = img.clamp(-1, 1)\n",
        "    return img\n",
        "\n",
        "example_prompts = [\n",
        "    \"a hand-drawn circle\",\n",
        "    \"a hand-drawn triangle\",\n",
        "    \"a hand-drawn square\",\n",
        "]\n",
        "\n",
        "# Uncomment after training\n",
        "samples = sample(example_prompts)\n",
        "grid = make_grid(samples, nrow=3, normalize=True, value_range=(-1, 1))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
