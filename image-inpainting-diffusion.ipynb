{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Inpainting with a Simple Diffusion Model\nThis notebook trains a minimal DDPM-style model for option 2 (image inpainting) using the same dataset and preprocessing as Task 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Reuse the same dataset path and image size from Task 1\n",
        "DATASET_PATH = \"data\"\n",
        "IMAGE_SIZE = 32\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-4\n",
        "TIMESTEPS = 200\n",
        "MASK_RATIO = 0.4  # central square size relative to image\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "class ShapeDataset(Dataset):\n",
        "    \"\"\"Same preprocessing as Task 1 for the circle/square/triangle sketches.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str = DATASET_PATH, img_size: int = IMAGE_SIZE, max_items_per_class: int = 2000, augment: bool = True):\n",
        "        self.samples = []\n",
        "        for name in [\"circle\", \"square\", \"triangle\"]:\n",
        "            path = os.path.join(data_dir, f\"{name}.npy\")\n",
        "            arr = np.load(path, mmap_mode=\"r\")\n",
        "            arr = arr[:max_items_per_class]\n",
        "            if arr.ndim == 4:  # (N,H,W,C)\n",
        "                arr = arr[..., 0]\n",
        "            self.samples.extend(arr.astype(np.float32))\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def _prepare(self, img: np.ndarray) -> torch.Tensor:\n",
        "        tensor = torch.as_tensor(img, dtype=torch.float32)\n",
        "        if tensor.ndim == 1:\n",
        "            side = int(math.sqrt(tensor.numel()))\n",
        "            tensor = tensor.view(side, side)\n",
        "        if tensor.ndim == 2:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        if tensor.shape[0] > 1:\n",
        "            tensor = tensor.mean(dim=0, keepdim=True)\n",
        "        return tensor\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        img = self._prepare(self.samples[idx])\n",
        "        img = TF.resize(img, [self.img_size, self.img_size])\n",
        "        if self.augment:\n",
        "            angle = random.uniform(-8, 8)\n",
        "            img = TF.rotate(img, angle, fill=0.0)\n",
        "            img = TF.affine(img, angle=0.0, translate=(random.uniform(-2, 2), random.uniform(-2, 2)), scale=1.0, shear=0.0, fill=0.0)\n",
        "        img = (img / 255.0).clamp(0, 1) * 2 - 1  # [-1, 1]\n",
        "        return img\n",
        "\n",
        "\n",
        "def central_square_mask(shape: Tuple[int, int], ratio: float = MASK_RATIO) -> torch.Tensor:\n",
        "    h, w = shape\n",
        "    mask = torch.zeros((1, h, w))\n",
        "    size = int(min(h, w) * ratio)\n",
        "    top = (h - size) // 2\n",
        "    left = (w - size) // 2\n",
        "    mask[:, top:top + size, left:left + size] = 1.0\n",
        "    return mask\n",
        "\n",
        "\n",
        "def apply_inpainting_mask(imgs: torch.Tensor, ratio: float = MASK_RATIO):\n",
        "    \"\"\"Return masked images and the corresponding binary mask (1 = hole).\"\"\"\n",
        "    b, c, h, w = imgs.shape\n",
        "    mask = central_square_mask((h, w), ratio).to(imgs.device).expand(b, -1, -1, -1)\n",
        "    noise = torch.randn_like(imgs)\n",
        "    masked = imgs * (1 - mask) + noise * mask\n",
        "    return masked, mask\n",
        "\n",
        "\n",
        "def get_dataloader(batch_size=BATCH_SIZE, img_size=IMAGE_SIZE, max_items_per_class=2000, augment=True):\n",
        "    dataset = ShapeDataset(img_size=img_size, max_items_per_class=max_items_per_class, augment=augment)\n",
        "    print(f\"Loaded {len(dataset)} sketches\")\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "\n",
        "dataloader = get_dataloader()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Peek at a few masked samples\n",
        "imgs = next(iter(dataloader))[:6]\n",
        "masked_imgs, masks = apply_inpainting_mask(imgs, ratio=MASK_RATIO)\n",
        "fig, axes = plt.subplots(2, 6, figsize=(12, 4))\n",
        "for i in range(6):\n",
        "    axes[0, i].imshow(((imgs[i].squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "    axes[1, i].imshow(((masked_imgs[i].squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "axes[0, 0].set_ylabel(\"original\")\n",
        "axes[1, 0].set_ylabel(\"masked\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def sinusoidal_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n",
        "    device = timesteps.device\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=device) / half)\n",
        "    angles = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
        "    if dim % 2 == 1:\n",
        "        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
        "    return emb\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_dim):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(8, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(8, out_ch),\n",
        "            nn.SiLU(),\n",
        "        )\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_dim, out_ch), nn.SiLU())\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        h = self.conv(x)\n",
        "        t = self.time_mlp(t_emb)[:, :, None, None]\n",
        "        return h + t\n",
        "\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, img_channels=1, base=32, time_dim=128):\n",
        "        super().__init__()\n",
        "        in_channels = img_channels + 1  # image + mask\n",
        "        self.time_dim = time_dim\n",
        "        self.inc = ConvBlock(in_channels, base, time_dim)\n",
        "        self.down = ConvBlock(base, base * 2, time_dim)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.bot = ConvBlock(base * 2, base * 2, time_dim)\n",
        "        self.up = nn.ConvTranspose2d(base * 2, base, 2, stride=2)\n",
        "        self.dec = ConvBlock(base * 2, base, time_dim)\n",
        "        self.outc = nn.Conv2d(base, img_channels, 1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = sinusoidal_embedding(t, self.time_dim)\n",
        "        x1 = self.inc(x, t_emb)\n",
        "        x2 = self.down(self.pool(x1), t_emb)\n",
        "        x3 = self.bot(self.pool(x2), t_emb)\n",
        "        x = self.up(x3)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.dec(x, t_emb)\n",
        "        return self.outc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "class Diffusion:\n",
        "    def __init__(self, timesteps: int = TIMESTEPS, device: torch.device = device):\n",
        "        self.timesteps = timesteps\n",
        "        self.device = device\n",
        "        self.betas = torch.linspace(1e-4, 0.02, timesteps, device=device)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "\n",
        "    def q_sample(self, x0, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "        sqrt_ac = torch.sqrt(self.alpha_cumprod[t])[:, None, None, None]\n",
        "        sqrt_om = torch.sqrt(1 - self.alpha_cumprod[t])[:, None, None, None]\n",
        "        return sqrt_ac * x0 + sqrt_om * noise\n",
        "\n",
        "    def p_sample(self, model_out, x, t):\n",
        "        beta_t = self.betas[t][:, None, None, None]\n",
        "        sqrt_one_minus_ac = torch.sqrt(1 - self.alpha_cumprod[t])[:, None, None, None]\n",
        "        sqrt_recip_alpha = torch.sqrt(1.0 / self.alphas[t])[:, None, None, None]\n",
        "        model_mean = sqrt_recip_alpha * (x - beta_t / sqrt_one_minus_ac * model_out)\n",
        "        if (t == 0).all():\n",
        "            return model_mean\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(beta_t) * noise\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Instantiate model, optimizer, and diffusion utilities\n",
        "model = SimpleUNet().to(device)\n",
        "diffusion = Diffusion()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "def train_epoch(loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for imgs in tqdm(loader, desc=\"train\"):\n",
        "        imgs = imgs.to(device)\n",
        "        masked_imgs, mask = apply_inpainting_mask(imgs)\n",
        "        b = imgs.size(0)\n",
        "        t = torch.randint(0, diffusion.timesteps, (b,), device=device).long()\n",
        "        noise = torch.randn_like(imgs)\n",
        "        x_t = diffusion.q_sample(imgs, t, noise)\n",
        "        x_t_masked = x_t * (1 - mask) + torch.randn_like(x_t) * mask\n",
        "        net_inp = torch.cat([x_t_masked, mask], dim=1)\n",
        "        pred_noise = model(net_inp, t)\n",
        "        loss = nn.functional.mse_loss(pred_noise, noise)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * b\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    avg_loss = train_epoch(dataloader)\n",
        "    print(f\"Epoch {epoch}: loss={avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def inpaint(imgs, ratio: float = MASK_RATIO, steps: int = TIMESTEPS):\n",
        "    model.eval()\n",
        "    mask_imgs, mask = apply_inpainting_mask(imgs.to(device), ratio)\n",
        "    b = imgs.size(0)\n",
        "    img = torch.randn_like(mask_imgs)\n",
        "    for i in reversed(range(steps)):\n",
        "        t = torch.full((b,), i, device=device, dtype=torch.long)\n",
        "        combined = img * mask + mask_imgs * (1 - mask)\n",
        "        net_inp = torch.cat([combined, mask], dim=1)\n",
        "        pred_noise = model(net_inp, t)\n",
        "        beta_t = diffusion.betas[t][:, None, None, None]\n",
        "        sqrt_one_minus_ac = torch.sqrt(1 - diffusion.alpha_cumprod[t])[:, None, None, None]\n",
        "        sqrt_recip_alpha = torch.sqrt(1.0 / diffusion.alphas[t])[:, None, None, None]\n",
        "        model_mean = sqrt_recip_alpha * (img - beta_t / sqrt_one_minus_ac * pred_noise)\n",
        "        if i > 0:\n",
        "            noise = torch.randn_like(img)\n",
        "            img = model_mean + torch.sqrt(beta_t) * noise\n",
        "        else:\n",
        "            img = model_mean\n",
        "        # enforce known pixels\n",
        "        img = img * mask + mask_imgs * (1 - mask)\n",
        "    return img, mask_imgs, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Visualize a few inpainting results\n",
        "samples = next(iter(dataloader))[:6]\n",
        "recon, masked, mask = inpaint(samples, steps=50)\n",
        "\n",
        "fig, axes = plt.subplots(3, 6, figsize=(12, 6))\n",
        "for i in range(6):\n",
        "    axes[0, i].imshow(((samples[i].cpu().squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "    axes[1, i].imshow(((masked[i].cpu().squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "    axes[2, i].imshow(((recon[i].cpu().squeeze() + 1) / 2).clamp(0, 1), cmap=\"gray\")\n",
        "    axes[2, i].axis(\"off\")\n",
        "axes[0, 0].set_ylabel(\"original\")\n",
        "axes[1, 0].set_ylabel(\"masked\")\n",
        "axes[2, 0].set_ylabel(\"inpainted\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}