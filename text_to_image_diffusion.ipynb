{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text-to-Image Diffusion (COCO Captions)\n",
        "\n",
        "Ниже собран ноутбук, который за один запуск проходит через подготовку данных, обучение простого диффузионного U-Net и генерацию изображений по текстовому промпту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Датасет: картинки + подписи\n",
        "Описание: загружаем COCO Captions с Hugging Face, оставляем только картинки и текст подписи. Это обеспечивает пары image-caption, необходимые для условной генерации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Базовые зависимости\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "# Директория для кэша/данных\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Конфигурация\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загружаем COCO captions и оставляем только нужные столбцы\n",
        "raw_dataset = load_dataset('jxie/coco_captions')\n",
        "columns_to_remove = [c for c in raw_dataset['train'].column_names if c not in ['image', 'captions']]\n",
        "dataset = raw_dataset.remove_columns(columns_to_remove)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Препроцессинг текста и картинок\n",
        "Описание: приводим изображения к размеру 256×256 и нормализуем в тензоры. Текст опускаем в lower, токенизируем CLIP-токенизатором, создаём input_ids и attention_mask. На выходе готовый Dataset/DataLoader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Трансформации для изображений\n",
        "to_tensor = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "# Токенизатор\n",
        "TOKENIZER_NAME = 'openai/clip-vit-base-patch32'\n",
        "tokenizer = CLIPTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "\n",
        "class CocoTextImageDataset(Dataset):\n",
        "    def __init__(self, hf_dataset, tokenizer, transform, max_length=64):\n",
        "        self.ds = hf_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.ds[idx]\n",
        "        image = item['image']\n",
        "        if not isinstance(image, Image.Image):\n",
        "            image = Image.open(image).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        caption = item['captions'][0].lower()\n",
        "        tokens = self.tokenizer(\n",
        "            caption,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'image': image,\n",
        "            'input_ids': tokens.input_ids.squeeze(0),\n",
        "            'attention_mask': tokens.attention_mask.squeeze(0),\n",
        "        }\n",
        "\n",
        "# Небольшой сэмпл для демонстрации\n",
        "train_split = dataset['train'].select(range(200))\n",
        "train_dataset = CocoTextImageDataset(train_split, tokenizer, to_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(batch['image'].shape, batch['input_ids'].shape, batch['attention_mask'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Простая модель (U-Net + текстовый энкодер)\n",
        "Описание: реализуем упрощённый диффузионный U-Net, который предсказывает шум. Текстовые эмбеддинги получаем CLIPTextModel и добавляем к временным эмбеддингам U-Net через конкатенацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Диффузионный график (beta schedule) и вспомогательные функции\n",
        "class DiffusionSchedule:\n",
        "    def __init__(self, timesteps=200, beta_start=1e-4, beta_end=0.02):\n",
        "        self.timesteps = timesteps\n",
        "        self.beta = torch.linspace(beta_start, beta_end, timesteps)\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        self.alpha_cumprod = torch.cumprod(self.alpha, dim=0)\n",
        "\n",
        "    def add_noise(self, x0, noise, t):\n",
        "        sqrt_alpha = torch.sqrt(self.alpha_cumprod[t])[:, None, None, None]\n",
        "        sqrt_one_minus_alpha = torch.sqrt(1 - self.alpha_cumprod[t])[:, None, None, None]\n",
        "        return sqrt_alpha * x0 + sqrt_one_minus_alpha * noise\n",
        "\n",
        "schedule = DiffusionSchedule(timesteps=200)\n",
        "\n",
        "# Вспомогательные модули\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, timesteps):\n",
        "        device = timesteps.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = timesteps[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, cond_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.time_proj = nn.Linear(cond_dim, out_channels)\n",
        "        self.act = nn.SiLU()\n",
        "        self.res_conv = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        h = self.conv1(x)\n",
        "        h = h + self.time_proj(cond)[:, :, None, None]\n",
        "        h = self.act(h)\n",
        "        h = self.conv2(h)\n",
        "        return self.act(h + self.res_conv(x))\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, base_channels=64, cond_dim=768, time_dim=128):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_dim),\n",
        "            nn.Linear(time_dim, time_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_dim, time_dim)\n",
        "        )\n",
        "        self.text_proj = nn.Linear(cond_dim, time_dim)\n",
        "\n",
        "        self.down1 = ResidualBlock(3, base_channels, time_dim)\n",
        "        self.down2 = ResidualBlock(base_channels, base_channels * 2, time_dim)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "        self.mid = ResidualBlock(base_channels * 2, base_channels * 2, time_dim)\n",
        "\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.up1 = ResidualBlock(base_channels * 2, base_channels, time_dim)\n",
        "        self.out = nn.Conv2d(base_channels, 3, 1)\n",
        "\n",
        "    def forward(self, x, t, text_emb):\n",
        "        t_emb = self.time_mlp(t)\n",
        "        cond = t_emb + self.text_proj(text_emb)\n",
        "\n",
        "        d1 = self.down1(x, cond)\n",
        "        d2 = self.down2(self.pool(d1), cond)\n",
        "        mid = self.mid(d2, cond)\n",
        "        u1 = self.up(mid)\n",
        "        u1 = self.up1(u1 + d1, cond)\n",
        "        return self.out(u1)\n",
        "\n",
        "# Текстовый энкодер CLIP\n",
        "text_encoder = CLIPTextModel.from_pretrained(TOKENIZER_NAME).to(device)\n",
        "model = SimpleUNet().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Тренировка (loss, цикл обучения)\n",
        "Описание: добавляем шум к реальным изображениям, U-Net предсказывает шум. Используем MSE loss между предсказанным и истинным шумом. Цикл обучения демонстрирует несколько шагов для примера."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "@torch.no_grad\n",
        "def encode_text(input_ids, attention_mask):\n",
        "    enc = text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    # Используем CLS-токен (первый скрытый слой) как эмбеддинг\n",
        "    return enc.last_hidden_state[:, 0]\n",
        "\n",
        "num_steps = 5  # демо-значение, при реальном обучении увеличить\n",
        "model.train()\n",
        "for step in range(num_steps):\n",
        "    for batch in train_loader:\n",
        "        imgs = batch['image'].to(device)\n",
        "        ids = batch['input_ids'].to(device)\n",
        "        mask = batch['attention_mask'].to(device)\n",
        "        t = torch.randint(0, schedule.timesteps, (imgs.size(0),), device=device)\n",
        "        noise = torch.randn_like(imgs)\n",
        "        noisy = schedule.add_noise(imgs, noise, t)\n",
        "        text_emb = encode_text(ids, mask)\n",
        "        pred = model(noisy, t, text_emb)\n",
        "        loss = nn.functional.mse_loss(pred, noise)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Step {step+1}/{num_steps} - loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Генерация картинок по промпту\n",
        "Описание: начиная с случайного шума, выполняем обратный диффузионный процесс, постепенно убирая шум с учётом текстового эмбеддинга. В конце визуализируем результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad\n",
        "def p_sample(model, x, t, text_emb):\n",
        "    beta_t = schedule.beta[t][:, None, None, None].to(device)\n",
        "    alpha_t = schedule.alpha[t][:, None, None, None].to(device)\n",
        "    alpha_bar = schedule.alpha_cumprod[t][:, None, None, None].to(device)\n",
        "\n",
        "    pred_noise = model(x, t, text_emb)\n",
        "    mean = (1 / torch.sqrt(alpha_t)) * (x - beta_t / torch.sqrt(1 - alpha_bar) * pred_noise)\n",
        "    noise = torch.randn_like(x) if (t > 0).all() else torch.zeros_like(x)\n",
        "    return mean + torch.sqrt(beta_t) * noise\n",
        "\n",
        "@torch.no_grad\n",
        "def generate(prompt, steps=50):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=64)\n",
        "    text_emb = encode_text(tokens.input_ids.to(device), tokens.attention_mask.to(device))\n",
        "    x = torch.randn(1, 3, 256, 256, device=device)\n",
        "    for i in reversed(range(steps)):\n",
        "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
        "        x = p_sample(model, x, t, text_emb)\n",
        "    x = (x.clamp(-1, 1) + 1) / 2\n",
        "    return x\n",
        "\n",
        "sample = generate(\"a cat sitting on a sofa\", steps=20)\n",
        "print('Generated sample shape:', sample.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Итоги\n",
        "Ноутбук загружает COCO captions, подготавливает изображения и текст, обучает условный U-Net предсказывать шум и умеет генерировать изображение по промпту."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}