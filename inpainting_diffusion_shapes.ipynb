{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Inpainting with Diffusion on Simple Shapes\n",
    "\n",
    "This notebook builds a diffusion-based image inpainting model for simple grayscale shapes. Image inpainting fills in missing regions of an image; here we use a denoising diffusion process together with a small U-Net to reconstruct masked portions. The dataset contains 32\u00d732 single-channel images of circles, squares, and triangles stored as NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Dataset and preprocessing](#Dataset-and-preprocessing)\n",
    "- [Inpainting mask](#Inpainting-mask)\n",
    "- [Diffusion model and U-Net](#Diffusion-model-and-U-Net)\n",
    "- [Training loop](#Training-loop)\n",
    "- [Inpainting and evaluation](#Inpainting-and-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and preprocessing\n",
    "\n",
    "Images are loaded from `.npy` files (`circle.npy`, `square.npy`, `triangle.npy`) located in the `data` folder. Each image is grayscale and reshaped to a tensor of shape `(1, 32, 32)`, converted to `float32`, and normalized to the range `[\u22121, 1]`. An optional limit `max_items_per_class` allows sub-sampling each shape class. A `DataLoader` is created with the configured batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths and hyperparameters\n",
    "DATASET_PATH = \"data\"\n",
    "IMAGE_SIZE = 32\n",
    "CHANNELS = 1\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 30\n",
    "TIMESTEPS = 300\n",
    "MASK_RATIO = 0.5\n",
    "max_items_per_class = None  # set an integer to limit samples per shape\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def to_tensor(img: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Convert a numpy image to a normalized torch tensor in [-1, 1].\"\"\"\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    if img.ndim == 3 and img.shape[-1] == 1:\n",
    "        img = img[..., 0]\n",
    "    if img.ndim == 1:\n",
    "        side = int(math.sqrt(img.size))\n",
    "        if side * side != img.size:\n",
    "            raise ValueError(f\"Cannot reshape flat image of size {img.size} into a square.\")\n",
    "        img = img.reshape(side, side)\n",
    "    if img.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D grayscale image, got shape {img.shape}.\")\n",
    "\n",
    "    tensor = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    tensor = F.interpolate(tensor, size=(IMAGE_SIZE, IMAGE_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "    tensor = tensor.squeeze(0)  # (1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    tensor = (tensor / 127.5) - 1.0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "class SimpleShapesDataset(Dataset):\n",
    "    \"\"\"Dataset loading simple shape numpy arrays and returning tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, root: str, max_items_per_class=None):\n",
    "        super().__init__()\n",
    "        self.samples = []\n",
    "        for name in [\"circle\", \"square\", \"triangle\"]:\n",
    "            path = os.path.join(root, f\"{name}.npy\")\n",
    "            data = np.load(path)\n",
    "            if max_items_per_class is not None:\n",
    "                data = data[:max_items_per_class]\n",
    "            self.samples.extend([to_tensor(x) for x in data])\n",
    "        self.samples = torch.stack(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# Load dataset and create dataloader\n",
    "dataset = SimpleShapesDataset(DATASET_PATH, max_items_per_class=max_items_per_class)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples on {DEVICE}.\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting mask\n",
    "\n",
    "We remove a central square region from each image using a binary mask with value `1` inside the missing area and `0` elsewhere. The `MASK_RATIO` controls the fraction of the image side covered by the square. The same mask is used during training and inference to hide the target region and to guide reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def central_square_mask(image_shape, ratio: float):\n",
    "    \"\"\"Create a central square mask with ones in the missing region.\"\"\"\n",
    "    _, h, w = image_shape\n",
    "    mask = torch.zeros(image_shape, dtype=torch.float32)\n",
    "    size_h, size_w = int(h * ratio), int(w * ratio)\n",
    "    start_h = (h - size_h) // 2\n",
    "    start_w = (w - size_w) // 2\n",
    "    mask[:, start_h : start_h + size_h, start_w : start_w + size_w] = 1.0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def apply_inpainting_mask(imgs: torch.Tensor, ratio: float = MASK_RATIO):\n",
    "    \"\"\"Apply a central mask to a batch of images and return masked images and mask.\"\"\"\n",
    "    b, c, h, w = imgs.shape\n",
    "    mask = central_square_mask((c, h, w), ratio).to(imgs.device)\n",
    "    mask = mask.unsqueeze(0).expand(b, -1, -1, -1)\n",
    "    # Zero out the masked region for visualization\n",
    "    masked_imgs = imgs * (1 - mask)\n",
    "    return masked_imgs, mask\n",
    "\n",
    "\n",
    "# Visualize a few masked samples\n",
    "batch = next(iter(loader))\n",
    "masked_batch, vis_mask = apply_inpainting_mask(batch)\n",
    "\n",
    "fig, axes = plt.subplots(2, 6, figsize=(12, 4))\n",
    "for i in range(6):\n",
    "    axes[0, i].imshow(batch[i].squeeze(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[0, i].set_title(\"Original\")\n",
    "    axes[1, i].imshow(masked_batch[i].squeeze(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "    axes[1, i].axis(\"off\")\n",
    "    axes[1, i].set_title(\"Masked\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion model and U-Net\n",
    "\n",
    "A Denoising Diffusion Probabilistic Model (DDPM) gradually adds Gaussian noise to an image through a forward process \\(q(x_t \\mid x_0)\\) controlled by a noise schedule. With pre-computed betas \\(\beta_t\\), we derive \\(\u0007lpha_t = 1 - \beta_t\\) and the cumulative product \\(\bar{\u0007lpha}_t\\). The forward sampling is\n",
    "\n",
    "\\[ x_t = \\sqrt{\bar{\u0007lpha}_t} x_0 + \\sqrt{1-\bar{\u0007lpha}_t}\\, \\epsilon. \\]\n",
    "\n",
    "The model learns to predict the noise \\(\\epsilon\\) given a noisy input. We concatenate the masked noisy image with the binary mask and provide a sinusoidal time embedding to a compact U-Net that outputs the predicted noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion utilities\n",
    "\n",
    "\n",
    "def sinusoidal_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    timesteps = timesteps.to(torch.float32)\n",
    "    device = timesteps.device\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(10000) * torch.arange(half, device=device).float() / (half - 1))\n",
    "    args = timesteps.unsqueeze(1) * freqs.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
    "    if dim % 2 == 1:\n",
    "        emb = torch.cat([emb, torch.zeros(timesteps.size(0), 1, device=device)], dim=1)\n",
    "    return emb\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, time_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_dim, out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.act(self.bn1(self.conv1(x)))\n",
    "        t_emb = self.time_mlp(t).view(t.size(0), -1, 1, 1)\n",
    "        h = h + t_emb\n",
    "        h = self.act(self.bn2(self.conv2(h)))\n",
    "        return h\n",
    "\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps: int, device: torch.device, beta_start: float = 1e-4, beta_end: float = 0.02):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def q_sample(self, x_start: torch.Tensor, t: torch.Tensor, noise: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_ab = torch.sqrt(self.alpha_cumprod[t]).view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_ab = torch.sqrt(1 - self.alpha_cumprod[t]).view(-1, 1, 1, 1)\n",
    "        return sqrt_ab * x_start + sqrt_one_minus_ab * noise\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_ch, out_ch, time_dim)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        h = self.conv(x, t)\n",
    "        p = self.pool(h)\n",
    "        return p, h  # pooled for next stage, h saved as skip\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_dim):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_ch + out_ch, out_ch, time_dim)\n",
    "\n",
    "    def forward(self, x, skip, t):\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x, t)\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, img_channels=1, time_dim=128):\n",
    "        super().__init__()\n",
    "        in_ch = img_channels + 1  # extra channel for mask\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        self.inc = ConvBlock(in_ch, 32, time_dim)\n",
    "        self.down1 = Down(32, 64, time_dim)\n",
    "        self.down2 = Down(64, 128, time_dim)\n",
    "\n",
    "        self.bot = ConvBlock(128, 128, time_dim)\n",
    "\n",
    "        self.up2 = Up(128, 128, time_dim)\n",
    "        self.up1 = Up(128, 64, time_dim)\n",
    "        self.outc = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, img_channels, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = sinusoidal_embedding(t, self.time_dim)\n",
    "\n",
    "        x1 = self.inc(x, t_emb)\n",
    "        x2, skip1 = self.down1(x1, t_emb)\n",
    "        x3, skip2 = self.down2(x2, t_emb)\n",
    "\n",
    "        bottleneck = self.bot(x3, t_emb)\n",
    "\n",
    "        x = self.up2(bottleneck, skip2, t_emb)\n",
    "        x = self.up1(x, skip1, t_emb)\n",
    "        return self.outc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Training samples a random timestep `t` and noise `\\epsilon ~ N(0, I)`, forms `x_t = q(x_t | x_0, t, \\epsilon)`, and builds a masked version that removes the original signal inside the hole:\n",
    "\n",
    "\\[ x^{\text{masked}}_t = x_t (1-\text{mask}) + \\sqrt{1-\bar{\u0007lpha}_t}\\, \\epsilon \\cdot \text{mask}. \\]\n",
    "\n",
    "This keeps the unmasked area consistent with `x_t` while ensuring the masked area contains only the same noise used in `q_sample`, eliminating leakage of `x_0`. The old code incorrectly injected fresh noise inside the mask, forcing the model to predict noise different from its input. Here the identical noise tensor is used for both `q_sample` and constructing `x^{\text{masked}}_t`, so the supervised target matches the visible input statistics. The loss emphasises the masked region while keeping a global term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diffusion = Diffusion(timesteps=TIMESTEPS, device=DEVICE)\n",
    "model = SimpleUNet(img_channels=CHANNELS, time_dim=128).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "def train_epoch(loader):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for imgs in tqdm(loader, leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        _, mask = apply_inpainting_mask(imgs, ratio=MASK_RATIO)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        b = imgs.size(0)\n",
    "        t = torch.randint(0, diffusion.timesteps, (b,), device=DEVICE)\n",
    "        noise = torch.randn_like(imgs)\n",
    "\n",
    "        x_t = diffusion.q_sample(imgs, t, noise)\n",
    "        alpha_bar_t = diffusion.alpha_cumprod[t].view(b, 1, 1, 1)\n",
    "        sqrt_one_minus_ac = torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        # Remove the clean component inside the mask; keep unmasked region intact\n",
    "        x_t_masked = x_t * (1 - mask) + sqrt_one_minus_ac * noise * mask\n",
    "\n",
    "        net_inp = torch.cat([x_t_masked, mask], dim=1)\n",
    "        pred_noise = model(net_inp, t)\n",
    "\n",
    "        masked_loss = F.mse_loss(pred_noise * mask, noise * mask, reduction=\"sum\") / (mask.sum() + 1e-8)\n",
    "        global_loss = mse_loss(pred_noise, noise)\n",
    "        loss = 0.7 * masked_loss + 0.3 * global_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "def train(epochs=EPOCHS):\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(loader)\n",
    "        losses.append(loss)\n",
    "        print(f\"Epoch {epoch:03d} | loss: {loss:.4f}\")\n",
    "    return losses\n",
    "\n",
    "# Uncomment to train (may take a few minutes depending on hardware)\n",
    "# training_losses = train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting and evaluation\n",
    "\n",
    "To inpaint, we start from random noise and iterate the reverse diffusion steps. At each timestep we:\n",
    "\n",
    "1. Hide the masked region with fresh noise and keep the known pixels.\n",
    "2. Predict the noise with the U-Net and compute \\(x_{t-1}\\) using the DDPM reverse formula.\n",
    "3. Enforce the known pixels outside the mask to match the original image.\n",
    "4. Blend the update with the predicted clean image using a guidance weight.\n",
    "\n",
    "We visualise the original, masked, and inpainted images, and optionally report the mean squared error inside the masked region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_step(x, t, eps_pred, diffusion: Diffusion):\n",
    "    beta_t = diffusion.betas[t]\n",
    "    alpha_t = diffusion.alphas[t]\n",
    "    alpha_bar_t = diffusion.alpha_cumprod[t]\n",
    "    sqrt_one_minus_ab = torch.sqrt(1 - alpha_bar_t)\n",
    "    sqrt_recip_alpha = torch.sqrt(1 / alpha_t)\n",
    "\n",
    "    coeff = (1 - alpha_t) / sqrt_one_minus_ab\n",
    "    mean = sqrt_recip_alpha * (x - coeff * eps_pred)\n",
    "\n",
    "    if t == 0:\n",
    "        return mean\n",
    "    noise = torch.randn_like(x)\n",
    "    return mean + torch.sqrt(beta_t) * noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def inpaint(imgs: torch.Tensor, ratio: float = MASK_RATIO, steps: int = TIMESTEPS, guide_strength: float = 0.3):\n",
    "    model.eval()\n",
    "    imgs = imgs.to(DEVICE)\n",
    "    b = imgs.size(0)\n",
    "    masked_imgs, mask = apply_inpainting_mask(imgs, ratio=ratio)\n",
    "\n",
    "    x = torch.randn_like(imgs)  # start from pure noise\n",
    "    for step in reversed(range(steps)):\n",
    "        t = torch.full((b,), step, device=DEVICE, dtype=torch.long)\n",
    "        alpha_bar_t = diffusion.alpha_cumprod[t].view(b, 1, 1, 1)\n",
    "        sqrt_one_minus_ab = torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        # Hide the target region with matching noise statistics\n",
    "        masked_noise = torch.randn_like(x) * sqrt_one_minus_ab\n",
    "        x_masked = x * (1 - mask) + masked_noise * mask\n",
    "        net_inp = torch.cat([x_masked, mask], dim=1)\n",
    "\n",
    "        eps_pred = model(net_inp, t)\n",
    "        x0_pred = (x - sqrt_one_minus_ab * eps_pred) / torch.sqrt(alpha_bar_t)\n",
    "\n",
    "        x = p_sample_step(x, step, eps_pred, diffusion)\n",
    "\n",
    "        # Keep known pixels close to the original\n",
    "        x = x * mask + imgs * (1 - mask)\n",
    "        # Blend with predicted clean image\n",
    "        x = (1 - guide_strength) * x + guide_strength * x0_pred\n",
    "\n",
    "    return x.clamp(-1, 1), masked_imgs, mask\n",
    "\n",
    "\n",
    "def denorm(x):\n",
    "    return (x + 1) / 2\n",
    "\n",
    "\n",
    "def visualize_inpainting(batch):\n",
    "    inpainted, masked_imgs, _ = inpaint(batch, ratio=MASK_RATIO, steps=TIMESTEPS)\n",
    "    batch = batch.cpu()\n",
    "    masked_imgs = masked_imgs.cpu()\n",
    "    inpainted = inpainted.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 6, figsize=(12, 6))\n",
    "    for i in range(6):\n",
    "        axes[0, i].imshow(denorm(batch[i]).squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[0, i].set_title(\"Original\")\n",
    "\n",
    "        axes[1, i].imshow(denorm(masked_imgs[i]).squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[1, i].axis(\"off\")\n",
    "        axes[1, i].set_title(\"Masked\")\n",
    "\n",
    "        axes[2, i].imshow(denorm(inpainted[i]).squeeze(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[2, i].axis(\"off\")\n",
    "        axes[2, i].set_title(\"Inpainted\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def masked_mse(inpainted, original, mask):\n",
    "    return F.mse_loss(inpainted * mask, original * mask)\n",
    "\n",
    "\n",
    "# Example usage (requires a trained model)\n",
    "# visualize_inpainting(next(iter(loader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "A compact diffusion model paired with a U-Net can inpaint missing regions in simple shape images. The critical fix was to ensure the same noise tensor is used both for generating \\(x_t\\) and for constructing the masked input; previously, unrelated noise inside the mask prevented learning. Visual inspection of masked versus reconstructed images, along with masked-region MSE, shows the model can recover the withheld central area after training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}