# Text-to-Image Diffusion: Prompt Preprocessing MVP

Эта ветка реализует первую половину задания: аккуратная подготовка текстовых
подсказок перед обучением текст-условленного диффузионного генератора.

## Что входит в MVP
- Нормализация и очистка текста (обрезка, опциональный lowercase, схлопывание
  пробелов).
- Токенизация любым совместимым токенизатором Hugging Face (по умолчанию
  `openai/clip-vit-base-patch32`, чтобы совпадать с CLIP-текст энкодером
  Stable Diffusion).
- Экспорт всех промежуточных данных (исходный текст, очищенный текст, токены,
  ids, attention mask) в JSONL для ручной проверки.

## Быстрый старт
Установите зависимости (нужен `transformers` с fast-tokenizer):
```bash
pip install -r requirements.txt
```

Пропустите примеры подсказок через препроцессор:
```bash
python -m src.text_preprocessing \
  --prompts "A sunset over snowy mountains" "Кот на скейтборде" \
  --output preprocessed_prompts.jsonl
```
Или считайте их из файла (txt или jsonl с полем `text`):
```bash
python -m src.text_preprocessing --input data/prompts.txt
```
Выход `preprocessed_prompts.jsonl` можно открыть любым редактором и убедиться,
что строки очищены и корректно токенизируются. Если нужно сохранить регистр,
добавьте флаг `--no-lowercase`.

## Рекомендации по моделям для следующих шагов
- **Текстовый энкодер**: `openai/clip-vit-base-patch32` или `laion/CLIP-ViT-H` —
  оба совместимы с распространёнными SD-пайплайнами и дают устоявшееся
  пространство эмбеддингов.
- **Дифузионная модель / U-Net**: начать со стабильных открытых весов
  (`stabilityai/stable-diffusion-2-base`) и адаптировать под своё разрешение.
  Для обучения с нуля использовать стандартный U-Net из `diffusers`, включив
  cross-attention блоки.
- **Варианты датасетов**: для быстрой проверки возьмите COCO captions (112k
  пар), позже масштабируйте до LAION или своего набора. Добавьте `train/val`
  сплит и фильтрацию по длине описаний.

## Что делать дальше (вторая половина задания)
1. **Подготовка изображений**: ресайз до выбранного разрешения (например,
   256×256), нормировка, сохранение в WebDataset/LMDB для потоковой загрузки.
2. **Связывание с текстом**: написать `Dataset`/`DataLoader`, который возвращает
   `(image_tensor, token_ids, attention_mask)` из файлов и JSONL с подписями.
3. **Определение шума и расписаний**: реализовать forward diffusion (добавление
   шума) и выбрать расписание бет (линейное/косинусное). Использовать
   `torch.linspace` или функции из `diffusers.schedulers`.
4. **Обучение U-Net**: реализовать лосс (обычно MSE по предсказанному шуму),
   добавить EMA весов, градиентный клиппинг, mixed precision.
5. **Обратный процесс**: собрать сэмплер (DDIM/PNDM/euler), подключить
   cross-attention на текстовые эмбеддинги и убедиться, что шаги денойзинга
   создают осмысленные изображения.
6. **Валидация**: сохранить примеры генерации для фиксированного набора промтов
   (которые уже прошли через текущий препроцессинг) и следить за метриками
   (CLIP-score/FID).

## Полезные проверки для первой части
- Просмотреть `preprocessed_prompts.jsonl` и убедиться, что все строки
  очищены, нет пустых токенов и длина не превышает `max_length`.
- Поменять токенизатор (например, на `bert-base-uncased`) и сравнить выход,
  чтобы выбрать оптимальный для вашей модели.
- Добавить в текст эмодзи/UTF-символы и убедиться, что fast-tokenizer корректно
  их обрабатывает или дополнительно включает `strip_accents`.

## Структура
- `src/text_preprocessing.py` — модуль и CLI для очистки/токенизации.
- `data/` — сюда можно складывать исходные промты/подписи.
- `PMLDL Assignment 3_ Diffusion - Yonote.pdf` — формулировка задания.
