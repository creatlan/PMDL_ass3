{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Text-to-Image Diffusion Demo\n",
    "\n",
    "This notebook walks through a tiny diffusion-like pipeline aligned with the provided text-to-image generation steps. The code uses lightweight components and synthetic data so it can run quickly while still showing the full process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "\n",
    "We treat the dataset as pairs of images and text prompts. The links you provided can be downloaded manually into a local folder named `data/`. Each row should include an image file and its caption. For simplicity, the demo below builds a synthetic dataset with small random images and simple text labels so that the notebook runs without external downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny synthetic dataset\n",
    "\n",
    "The dataset yields 32x32 RGB tensors and numeric tokens derived from the prompt. Replace the synthetic data with real images and captions from the provided Google Drive links by adjusting the `ImageTextDataset` class to read files instead of generating noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, prompts: List[str], image_size: int = 32):\n",
    "        self.prompts = prompts\n",
    "        self.image_size = image_size\n",
    "        self.transform = T.Compose([T.Resize((image_size, image_size)), T.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def _tokenize(self, text: str) -> torch.Tensor:\n",
    "        # Very small tokenizer: map characters to integer ids and pad/truncate.\n",
    "        tokens = [ord(c) % 256 for c in text.lower()][:16]\n",
    "        tokens += [0] * (16 - len(tokens))\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        prompt = self.prompts[idx]\n",
    "        # Synthetic image: random noise. Replace with real image loading if available.\n",
    "        img = torch.rand(3, self.image_size, self.image_size)\n",
    "        tokens = self._tokenize(prompt)\n",
    "        return img, tokens\n",
    "\n",
    "# Example prompts; swap these with captions from the shared dataset.\n",
    "prompts = [\n",
    "    \"a calm sunset over low mountains\",\n",
    "    \"bright flowers in a glass vase\",\n",
    "    \"a small orange cat resting on a pillow\",\n",
    "    \"city skyline at night with lights\",\n",
    "]\n",
    "train_ds = ImageTextDataset(prompts)\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model components\n",
    "\n",
    "The model couples a tiny text encoder with a light U-Net. The text encoder embeds tokens and averages them. The U-Net uses simple convolutions and injects the text embedding as a bias term during denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int = 256, embed_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        emb = self.embed(tokens)  # (B, T, D)\n",
    "        return emb.mean(dim=1)    # (B, D)\n",
    "\n",
    "class TinyUNet(nn.Module):\n",
    "    def __init__(self, text_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "        )\n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
    "        )\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "        )\n",
    "        self.text_to_bias = nn.Linear(text_dim, 64)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, text_emb: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.down(x)\n",
    "        bias = self.text_to_bias(text_emb).view(text_emb.size(0), -1, 1, 1)\n",
    "        h = self.mid(h + bias)\n",
    "        out = self.up(h)\n",
    "        return out\n",
    "\n",
    "text_encoder = SimpleTextEncoder()\n",
    "denoiser = TinyUNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion utilities\n",
    "\n",
    "The forward process adds Gaussian noise according to a beta schedule, and the reverse process asks the U-Net to predict the added noise. Here we keep only a few steps for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 10\n",
    "betas = torch.linspace(1e-4, 0.02, timesteps)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "def add_noise(x0: torch.Tensor, t: torch.Tensor, noise: torch.Tensor) -> torch.Tensor:\n",
    "    sqrt_ab = torch.sqrt(alpha_bars[t])[:, None, None, None]\n",
    "    sqrt_one_minus = torch.sqrt(1 - alpha_bars[t])[:, None, None, None]\n",
    "    return sqrt_ab * x0 + sqrt_one_minus * noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop (tiny demo)\n",
    "\n",
    "We train for a couple of epochs to show the mechanics. For a real project, increase the data size, epochs, and model capacity. Loss measures how well the model predicts the injected noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "text_encoder.to(device)\n",
    "denoiser.to(device)\n",
    "optimizer = torch.optim.Adam(list(text_encoder.parameters()) + list(denoiser.parameters()), lr=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for x0, tokens in train_dl:\n",
    "        x0 = x0.to(device)\n",
    "        tokens = tokens.to(device)\n",
    "        t = torch.randint(0, timesteps, (x0.size(0),), device=device)\n",
    "        noise = torch.randn_like(x0)\n",
    "        noisy = add_noise(x0, t, noise)\n",
    "        text_emb = text_encoder(tokens)\n",
    "        pred_noise = denoiser(noisy, text_emb)\n",
    "        loss = nn.functional.mse_loss(pred_noise, noise)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}: loss={loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from noise\n",
    "\n",
    "Starting from pure noise, we iteratively denoise using the learned model. The function below performs a handful of reverse steps and returns a generated image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(prompt: str) -> torch.Tensor:\n",
    "    denoiser.eval()\n",
    "    text_encoder.eval()\n",
    "    x = torch.randn(1, 3, train_ds.image_size, train_ds.image_size, device=device)\n",
    "    tokens = train_ds._tokenize(prompt).unsqueeze(0).to(device)\n",
    "    text_emb = text_encoder(tokens)\n",
    "    for i in reversed(range(timesteps)):\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        noise_pred = denoiser(x, text_emb)\n",
    "        alpha = alphas[i]\n",
    "        alpha_bar = alpha_bars[i]\n",
    "        if i > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            x = (1/torch.sqrt(alpha)) * (x - (1 - alpha) / torch.sqrt(1 - alpha_bar) * noise_pred) + torch.sqrt(betas[i]) * noise\n",
    "        else:\n",
    "            x = (1/torch.sqrt(alpha)) * (x - (1 - alpha) / torch.sqrt(1 - alpha_bar) * noise_pred)\n",
    "    return x.clamp(0, 1).cpu().squeeze(0)\n",
    "\n",
    "sample_image = sample('a calm sunset over low mountains')\n",
    "plt.imshow(sample_image.permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
